{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e59eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Example script for defining and using custom models in AutoGluon Tabular \"\"\"\n",
    "\n",
    "from autogluon.core.utils import infer_problem_type\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from autogluon.tabular.configs.hyperparameter_configs import get_hyperparameter_config\n",
    "from autogluon.core.data import LabelCleaner\n",
    "from autogluon.core.models import AbstractModel\n",
    "\n",
    "#########################\n",
    "# Create a custom model #\n",
    "#########################\n",
    "\n",
    "# In this example, we create a custom Naive Bayes model for use in AutoGluon\n",
    "class NaiveBayesModel(AbstractModel):\n",
    "    # The `_preprocess` method takes the input data and transforms it to the internal representation usable by the model.\n",
    "    # `_preprocess` is called by `preprocess` and is used during model fit and model inference.\n",
    "    def _preprocess(self, X, **kwargs):\n",
    "        # Drop category and object column dtypes, since NaiveBayes can't handle these dtypes.\n",
    "        cat_columns = X.select_dtypes(['category', 'object']).columns\n",
    "        X = X.drop(cat_columns, axis=1)\n",
    "        # Add a fillna call to handle missing values.\n",
    "        return super()._preprocess(X, **kwargs).fillna(0)\n",
    "\n",
    "    # The `_fit` method takes the input training data (and optionally the validation data) and trains the model.\n",
    "    def _fit(self, X, y, **kwargs):\n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "        # It is important to call `preprocess(X)` in `_fit` to replicate what will occur during inference.\n",
    "        X = self.preprocess(X)\n",
    "        self.model = GaussianNB(**self.params)\n",
    "        self.model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416ad247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a more optimized implementation that drops the invalid features earlier on to avoid having to make repeated checks.\n",
    "class AdvancedNaiveBayesModel(AbstractModel):\n",
    "    def _preprocess(self, X, **kwargs):\n",
    "        # Add a fillna call to handle missing values.\n",
    "        return super()._preprocess(X, **kwargs).fillna(0)\n",
    "\n",
    "    def _fit(self, X, y, **kwargs):\n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "        X = self.preprocess(X)\n",
    "        self.model = GaussianNB(**self.params)\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    # The `_get_default_auxiliary_params` method defines various model-agnostic parameters such as maximum memory usage and valid input column dtypes.\n",
    "    # For most users who build custom models, they will only need to specify the valid/invalid dtypes to the model here.\n",
    "    def _get_default_auxiliary_params(self) -> dict:\n",
    "        default_auxiliary_params = super()._get_default_auxiliary_params()\n",
    "        extra_auxiliary_params = dict(\n",
    "            # Drop category and object column dtypes, since NaiveBayes can't handle these dtypes.\n",
    "            ignored_type_group_raw=['category', 'object'],\n",
    "        )\n",
    "        default_auxiliary_params.update(extra_auxiliary_params)\n",
    "        return default_auxiliary_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a25f66ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_channels,seq_len):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # Input layer\n",
    "        self.conv1 = nn.Conv1d(input_channels, 4, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(4)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(4, 16, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        \n",
    "        # Hidden layer\n",
    "        self.flatten = nn.Flatten()\n",
    "        #self.fc1 = nn.Linear(440192, 128)\n",
    "        self.fc1 = nn.Linear(32*seq_len, 128)\n",
    "        self.bn5 = nn.BatchNorm1d(128)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 16)\n",
    "        self.bn6 = nn.BatchNorm1d(16)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class ConvNetModel(AbstractModel):\n",
    "    # The `_preprocess` method takes the input data and transforms it to the internal representation usable by the model.\n",
    "    # `_preprocess` is called by `preprocess` and is used during model fit and model inference.\n",
    "    def _preprocess(self, X, **kwargs):\n",
    "        # Perform preprocessing steps such as imputation, scaling, and one-hot encoding.\n",
    "        # Add any custom preprocessing steps if needed.\n",
    "        return super()._preprocess(X, **kwargs)\n",
    "\n",
    "          \n",
    "            \n",
    "    def _fit(self, X, y, batch_size=32, epochs=10, lr=0.001,  **kwargs):\n",
    "        X_train = torch.tensor(X.values, dtype=torch.float32)\n",
    "        y_train = torch.tensor(y.values, dtype=torch.float32)\n",
    "        #print(\"X_train.shape\",X_train.shape)\n",
    "        seq_len = X_train.shape[1]\n",
    "        print(\"seq_len\",seq_len)\n",
    "        self.model = ConvNet(input_channels = 1, seq_len=seq_len)\n",
    "        self.train(self.model, X_train, y_train, batch_size=batch_size, epochs=epochs, lr=lr)\n",
    "\n",
    "            \n",
    "    def predict(self, X):\n",
    "        X_test = torch.tensor(X.values, dtype=torch.float32)\n",
    "        X_test = X_test.unsqueeze(1)  # Adjust the input shape for Conv1d\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_test)\n",
    "            preds = torch.round(outputs)\n",
    "            predictions = preds.squeeze().tolist()\n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_test = torch.tensor(X.values, dtype=torch.float32)\n",
    "        X_test = X_test.unsqueeze(1)  # Adjust the input shape for Conv1d\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_test)\n",
    "            predictions = outputs.squeeze().tolist()\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5fb7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed8a09f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Loading Data #\n",
    "################\n",
    "\n",
    "# train_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')  # can be local CSV file as well, returns Pandas DataFrame\n",
    "# test_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')  # another Pandas DataFrame\n",
    "# label = 'class'  # specifies which column do we want to predict\n",
    "# train_data = train_data.head(1000)  # subsample for faster demo\n",
    "\n",
    "from autogluon.core.utils.loaders import load_pd\n",
    "import pandas as pd\n",
    "train_data  = load_pd.load('./train.csv')\n",
    "test_data  = load_pd.load('./test.csv')\n",
    "concatenated_df = pd.concat([train_data,test_data], axis=0)\n",
    "label = 'solubility'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c74f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import torch\n",
    "from autogluon.features.generators import AbstractFeatureGenerator\n",
    "from autogluon.common.features.types import R_INT,R_FLOAT,R_OBJECT,R_CATEGORY,S_TEXT_AS_CATEGORY \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class one_hot_Generator(AbstractFeatureGenerator):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _fit_transform(self, X: DataFrame, **kwargs) -> (DataFrame, dict):\n",
    "        # Here we can specify any logic we want to make a stateful feature generator based on the data.\n",
    "        # Just call _transform since this isn't a stateful feature generator.\n",
    "        X_out = self._transform(X)\n",
    "        # return the output and the new special types of the data. For this generator, we don't add any new special types, so just return the input special types\n",
    "        return X_out, self.feature_metadata_in.type_group_map_special\n",
    "\n",
    "    def _transform(self, X: DataFrame) -> DataFrame:\n",
    "        # Here we can specify the logic taken to convert input data to output data post-fit. Here we can reference any variables created during fit if the generator is stateful.\n",
    "        # Because this feature generator is not stateful, we simply add k to all features.\n",
    "        letter_to_int = {'C': 0, 'P': 1, 'R': 2, 'N': 3, 'F': 4, 'K': 5, 'A': 6, 'H': 7, 'Y': 8, 'V': 9, 'L': 10, 'D': 11, 'G': 12, 'E': 13, 'Q': 14, 'M': 15, 'T': 16, 'S': 17, 'I': 18, 'W': 19, 'X':20}\n",
    "\n",
    "        def one_hot_encoding(sequence,letter_to_int):\n",
    "            letter_sequence = [letter_to_int[letter] for letter in sequence]\n",
    "\n",
    "            encoded_tensor  = torch.zeros((len(letter_to_int),len(sequence)), dtype=torch.int64)\n",
    "            for i in range(len(letter_sequence)):\n",
    "                encoded_tensor[letter_sequence[i],i] = 1\n",
    "            return encoded_tensor\n",
    "        \n",
    "        # Convert the protein sequences to one-hot encoding\n",
    "        \n",
    "        one_hot_df = pd.DataFrame()\n",
    "\n",
    "        # get the first column \n",
    "        column = X.iloc[:, 0]\n",
    "        sequences = column.tolist()\n",
    "        one_hot_seqs = []\n",
    "        for seq in sequences:\n",
    "            one_hot_seq = one_hot_encoding(seq,letter_to_int)\n",
    "            one_hot_seqs.append(one_hot_seq.flatten().numpy())\n",
    "\n",
    "        #print(\"one_hot_seqs size\",one_hot_seqs.shape)\n",
    "        # Create a dataframe with separate columns for each amino acid position\n",
    "        max_length = max(len(seq) for seq in sequences)\n",
    "        column_name = [f'aa{i}_{aa}' for aa in letter_to_int for i in range(1, max_length+1) ]\n",
    "\n",
    "\n",
    "        #print(\"one_hot_seqs shape\",one_hot_seqs.shape)\n",
    "        df = pd.DataFrame(one_hot_seqs,columns = column_name)\n",
    "        df  = df.fillna(value=0).astype(\"bool\")\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_infer_features_in_args() -> dict:\n",
    "        default_infer_features = dict(valid_raw_types=[R_OBJECT]) \n",
    "        print(default_infer_features)\n",
    "        return default_infer_features  # This limits input features to only integers. We can assume that the input to _fit_transform and _transform only contain the data post-applying this filter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37befac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valid_raw_types': ['object']}\n",
      "      aa2_C  aa3_C  aa4_C  aa5_C  aa6_C  aa7_C  aa8_C  aa9_C  aa10_C  aa11_C  \\\n",
      "0     False  False  False  False  False  False  False  False   False   False   \n",
      "1     False  False  False  False  False  False  False  False   False   False   \n",
      "2     False  False  False  False  False  False  False  False   False   False   \n",
      "3     False  False  False  False  False  False  False  False   False   False   \n",
      "4     False  False  False  False  False  False  False  False   False   False   \n",
      "...     ...    ...    ...    ...    ...    ...    ...    ...     ...     ...   \n",
      "1318  False  False  False  False  False  False  False  False   False   False   \n",
      "1319  False  False  False  False  False  False  False  False   False   False   \n",
      "1320  False  False  False  False  False  False  False  False   False   False   \n",
      "1321  False  False  False  False  False  False  False  False   False   False   \n",
      "1322  False  False  False  False  False  False  False  False   False   False   \n",
      "\n",
      "      ...  aa576_W  aa627_W  aa635_W  aa739_W  aa745_W  aa749_W  aa776_W  \\\n",
      "0     ...    False    False    False    False    False    False    False   \n",
      "1     ...    False    False    False    False    False    False    False   \n",
      "2     ...    False    False    False    False    False    False    False   \n",
      "3     ...    False    False    False    False    False    False    False   \n",
      "4     ...    False    False    False    False    False    False    False   \n",
      "...   ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "1318  ...    False    False    False    False    False    False    False   \n",
      "1319  ...    False    False    False    False    False    False    False   \n",
      "1320  ...    False    False    False    False    False    False    False   \n",
      "1321  ...    False    False    False    False    False    False    False   \n",
      "1322  ...    False    False    False    False    False    False    False   \n",
      "\n",
      "      aa810_W  solubility  fold  \n",
      "0       False           1   0.0  \n",
      "1       False           1   1.0  \n",
      "2       False           1   1.0  \n",
      "3       False           1   1.0  \n",
      "4       False           1   2.0  \n",
      "...       ...         ...   ...  \n",
      "1318    False           1   NaN  \n",
      "1319    False           1   NaN  \n",
      "1320    False           1   NaN  \n",
      "1321    False           1   NaN  \n",
      "1322    False           1   NaN  \n",
      "\n",
      "[12547 rows x 13758 columns]\n"
     ]
    }
   ],
   "source": [
    "from autogluon.features.generators import CategoryFeatureGenerator, AsTypeFeatureGenerator, BulkFeatureGenerator, DropUniqueFeatureGenerator, FillNaFeatureGenerator, PipelineFeatureGenerator, OneHotEncoderFeatureGenerator,IdentityFeatureGenerator\n",
    "import copy\n",
    "import sys\n",
    "sys.path.append('/user/mahaohui/autoML/autogluon_examples')\n",
    "from feature_generator import count_charge_Generator, net_charge_Generator, one_hot_Generator\n",
    "\n",
    "\n",
    "train_feature_generator = PipelineFeatureGenerator(\n",
    "    generators=[\n",
    "        # Stage 1: Convert feature types to be the same as during fit. Does not need to be specified.\n",
    "        # Stage 2: Fill NaN values of data. Does not need to be specified.\n",
    "        [  # Stage 3: Add 5 to all int features and convert all object features to category features. Concatenate the outputs of each.\n",
    "            # count_charge_Generator(),\n",
    "            # net_charge_Generator(),\n",
    "            one_hot_Generator(verbosity=3,features_in=['seq']),\n",
    "            #OneHotEncoderFeatureGenerator(),\n",
    "            #CategoryFeatureGenerator(),\n",
    "            IdentityFeatureGenerator(infer_features_in_args=dict(\n",
    "                valid_raw_types=[R_INT, R_FLOAT])),\n",
    "        ],\n",
    "        # Stage 4: Drop any features which are always the same value (useless). Does not need to be specified.\n",
    "     ],\n",
    "    verbosity=3\n",
    ")\n",
    "one_hot_all_data = train_feature_generator.fit_transform(X=concatenated_df)\n",
    "print(one_hot_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "297ad5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       aa2_C  aa3_C  aa4_C  aa5_C  aa6_C  aa7_C  aa8_C  aa9_C  aa10_C  aa11_C  \\\n",
      "0      False  False  False  False  False  False  False  False   False   False   \n",
      "1      False  False  False  False  False  False  False  False   False   False   \n",
      "2      False  False  False  False  False  False  False  False   False   False   \n",
      "3      False  False  False  False  False  False  False  False   False   False   \n",
      "4      False  False  False  False  False  False  False  False   False   False   \n",
      "...      ...    ...    ...    ...    ...    ...    ...    ...     ...     ...   \n",
      "11219  False  False  False  False  False  False  False  False   False   False   \n",
      "11220  False  False  False  False  False  False  False  False   False   False   \n",
      "11221  False  False  False  False  False  False  False  False   False   False   \n",
      "11222  False  False  False  False  False  False  False  False   False   False   \n",
      "11223  False  False  False  False  False  False  False  False   False   False   \n",
      "\n",
      "       ...  aa576_W  aa627_W  aa635_W  aa739_W  aa745_W  aa749_W  aa776_W  \\\n",
      "0      ...    False    False    False    False    False    False    False   \n",
      "1      ...    False    False    False    False    False    False    False   \n",
      "2      ...    False    False    False    False    False    False    False   \n",
      "3      ...    False    False    False    False    False    False    False   \n",
      "4      ...    False    False    False    False    False    False    False   \n",
      "...    ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "11219  ...    False    False    False    False    False    False    False   \n",
      "11220  ...    False    False    False    False    False    False    False   \n",
      "11221  ...    False    False    False    False    False    False    False   \n",
      "11222  ...    False    False    False    False    False    False    False   \n",
      "11223  ...    False    False    False    False    False    False    False   \n",
      "\n",
      "       aa810_W  solubility  fold  \n",
      "0        False           1   0.0  \n",
      "1        False           1   1.0  \n",
      "2        False           1   1.0  \n",
      "3        False           1   1.0  \n",
      "4        False           1   2.0  \n",
      "...        ...         ...   ...  \n",
      "11219    False           1   0.0  \n",
      "11220    False           1   0.0  \n",
      "11221    False           1   0.0  \n",
      "11222    False           1   0.0  \n",
      "11223    False           0   3.0  \n",
      "\n",
      "[11224 rows x 13758 columns]\n",
      "      aa2_C  aa3_C  aa4_C  aa5_C  aa6_C  aa7_C  aa8_C  aa9_C  aa10_C  aa11_C  \\\n",
      "0     False  False  False  False  False  False  False  False   False   False   \n",
      "1     False  False  False  False  False  False  False  False   False   False   \n",
      "2     False  False  False  False  False  False  False  False   False   False   \n",
      "3     False  False  False  False  False  False  False  False   False   False   \n",
      "4     False  False  False  False  False  False  False  False   False   False   \n",
      "...     ...    ...    ...    ...    ...    ...    ...    ...     ...     ...   \n",
      "1318  False  False  False  False  False  False  False  False   False   False   \n",
      "1319  False  False  False  False  False  False  False  False   False   False   \n",
      "1320  False  False  False  False  False  False  False  False   False   False   \n",
      "1321  False  False  False  False  False  False  False  False   False   False   \n",
      "1322  False  False  False  False  False  False  False  False   False   False   \n",
      "\n",
      "      ...  aa546_W  aa576_W  aa627_W  aa635_W  aa739_W  aa745_W  aa749_W  \\\n",
      "0     ...    False    False    False    False    False    False    False   \n",
      "1     ...    False    False    False    False    False    False    False   \n",
      "2     ...    False    False    False    False    False    False    False   \n",
      "3     ...    False    False    False    False    False    False    False   \n",
      "4     ...    False    False    False    False    False    False    False   \n",
      "...   ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "1318  ...    False    False    False    False    False    False    False   \n",
      "1319  ...    False    False    False    False    False    False    False   \n",
      "1320  ...    False    False    False    False    False    False    False   \n",
      "1321  ...    False    False    False    False    False    False    False   \n",
      "1322  ...    False    False    False    False    False    False    False   \n",
      "\n",
      "      aa776_W  aa810_W  solubility  \n",
      "0       False    False           0  \n",
      "1       False    False           0  \n",
      "2       False    False           0  \n",
      "3       False    False           1  \n",
      "4       False    False           1  \n",
      "...       ...      ...         ...  \n",
      "1318    False    False           1  \n",
      "1319    False    False           1  \n",
      "1320    False    False           1  \n",
      "1321    False    False           1  \n",
      "1322    False    False           1  \n",
      "\n",
      "[1323 rows x 13757 columns]\n"
     ]
    }
   ],
   "source": [
    "one_hot_train_data = one_hot_all_data[:len(train_data)]\n",
    "one_hot_test_data = one_hot_all_data[len(train_data):]\n",
    "one_hot_test_data = one_hot_test_data.drop([\"fold\"],axis=1)\n",
    "print(one_hot_train_data)\n",
    "print(one_hot_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "965ee623",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_valid_data1 = one_hot_train_data[one_hot_train_data[\"fold\"] ==0.0]\n",
    "one_hot_train_data1 = one_hot_train_data[one_hot_train_data[\"fold\"] !=0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39d9dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_train_data1 = one_hot_train_data1.drop([\"fold\"],axis=1)\n",
    "one_hot_valid_data1 = one_hot_valid_data1.drop([\"fold\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0b9f49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       aa2_C  aa3_C  aa4_C  aa5_C  aa6_C  aa7_C  aa8_C  aa9_C  aa10_C  aa11_C  \\\n",
      "1      False  False  False  False  False  False  False  False   False   False   \n",
      "2      False  False  False  False  False  False  False  False   False   False   \n",
      "3      False  False  False  False  False  False  False  False   False   False   \n",
      "4      False  False  False  False  False  False  False  False   False   False   \n",
      "5      False  False  False  False  False  False  False  False   False   False   \n",
      "...      ...    ...    ...    ...    ...    ...    ...    ...     ...     ...   \n",
      "11213  False  False  False  False  False  False  False  False   False   False   \n",
      "11215  False  False  False  False  False  False  False  False   False   False   \n",
      "11216  False  False  False  False  False  False  False  False   False   False   \n",
      "11217  False  False  False  False  False  False  False  False   False   False   \n",
      "11223  False  False  False  False  False  False  False  False   False   False   \n",
      "\n",
      "       ...  aa546_W  aa576_W  aa627_W  aa635_W  aa739_W  aa745_W  aa749_W  \\\n",
      "1      ...    False    False    False    False    False    False    False   \n",
      "2      ...    False    False    False    False    False    False    False   \n",
      "3      ...    False    False    False    False    False    False    False   \n",
      "4      ...    False    False    False    False    False    False    False   \n",
      "5      ...    False    False    False    False    False    False    False   \n",
      "...    ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "11213  ...    False    False    False    False    False    False    False   \n",
      "11215  ...    False    False    False    False    False    False    False   \n",
      "11216  ...    False    False    False    False    False    False    False   \n",
      "11217  ...    False    False    False    False    False    False    False   \n",
      "11223  ...    False    False    False    False    False    False    False   \n",
      "\n",
      "       aa776_W  aa810_W  solubility  \n",
      "1        False    False           1  \n",
      "2        False    False           1  \n",
      "3        False    False           1  \n",
      "4        False    False           1  \n",
      "5        False    False           0  \n",
      "...        ...      ...         ...  \n",
      "11213    False    False           0  \n",
      "11215    False    False           1  \n",
      "11216    False    False           0  \n",
      "11217    False    False           0  \n",
      "11223    False    False           0  \n",
      "\n",
      "[8281 rows x 13757 columns]\n",
      "       aa2_C  aa3_C  aa4_C  aa5_C  aa6_C  aa7_C  aa8_C  aa9_C  aa10_C  aa11_C  \\\n",
      "0      False  False  False  False  False  False  False  False   False   False   \n",
      "10     False  False  False  False  False  False  False  False   False   False   \n",
      "21     False  False  False  False  False  False  False  False   False   False   \n",
      "61     False  False  False  False  False  False  False  False   False   False   \n",
      "69     False  False  False  False  False  False  False  False   False   False   \n",
      "...      ...    ...    ...    ...    ...    ...    ...    ...     ...     ...   \n",
      "11218  False  False  False  False  False  False  False  False   False   False   \n",
      "11219  False  False  False  False  False  False  False  False   False   False   \n",
      "11220  False  False  False  False  False  False  False  False   False   False   \n",
      "11221  False  False  False  False  False  False  False  False   False   False   \n",
      "11222  False  False  False  False  False  False  False  False   False   False   \n",
      "\n",
      "       ...  aa546_W  aa576_W  aa627_W  aa635_W  aa739_W  aa745_W  aa749_W  \\\n",
      "0      ...    False    False    False    False    False    False    False   \n",
      "10     ...    False    False    False    False    False    False    False   \n",
      "21     ...    False    False    False    False    False    False    False   \n",
      "61     ...    False    False    False    False    False    False    False   \n",
      "69     ...    False    False    False    False    False    False    False   \n",
      "...    ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "11218  ...    False    False    False    False    False    False    False   \n",
      "11219  ...    False    False    False    False    False    False    False   \n",
      "11220  ...    False    False    False    False    False    False    False   \n",
      "11221  ...    False    False    False    False    False    False    False   \n",
      "11222  ...    False    False    False    False    False    False    False   \n",
      "\n",
      "       aa776_W  aa810_W  solubility  \n",
      "0        False    False           1  \n",
      "10       False    False           0  \n",
      "21       False    False           1  \n",
      "61       False    False           1  \n",
      "69       False    False           1  \n",
      "...        ...      ...         ...  \n",
      "11218    False    False           1  \n",
      "11219    False    False           1  \n",
      "11220    False    False           1  \n",
      "11221    False    False           1  \n",
      "11222    False    False           1  \n",
      "\n",
      "[2943 rows x 13757 columns]\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_train_data1)\n",
    "print(one_hot_valid_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3d47b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230627_104219/\"\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (8281 samples, 113.99 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230627_104219/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.0\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 10 13:32:12 UTC 2021\n",
      "Train Data Rows:    8281\n",
      "Train Data Columns: 13756\n",
      "Tuning Data Rows:    2943\n",
      "Tuning Data Columns: 13756\n",
      "Label Column: solubility\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting IdentityFeatureGenerator...\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "Data preprocessing and feature engineering runtime = 8.32s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'precision'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Custom Model Type Detected: <class '__main__.ConvNetModel'>\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: ConvNetModel ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len 13756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tWarning: Exception caused ConvNetModel to fail during training... Skipping this model.\n",
      "\t\t'ConvNet' object has no attribute 'model'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1502, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1447, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 703, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/tmp/ipykernel_2117/3397548642.py\", line 106, in _fit\n",
      "    self.model.fit(X, y, batch_size=batch_size, epochs=epochs, lr=lr)\n",
      "  File \"/tmp/ipykernel_2117/3397548642.py\", line 72, in fit\n",
      "    optimizer = optim.Adam(self.model.parameters(), lr)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1269, in __getattr__\n",
      "    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
      "AttributeError: 'ConvNet' object has no attribute 'model'\n",
      "No base models to train on, skipping auxiliary stack level 2...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "AutoGluon did not successfully train any models",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# custom_hyperparameters = {NaiveBayesModel: [{}, {'var_smoothing': 0.00001}, {'var_smoothing': 0.000002}]}  # Train 3 NaiveBayes models with different hyperparameters\u001b[39;00m\n\u001b[1;32m      3\u001b[0m predictor \u001b[38;5;241m=\u001b[39m TabularPredictor(label\u001b[38;5;241m=\u001b[39mlabel,eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mone_hot_train_data1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtuning_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mone_hot_valid_data1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_hyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeature_generator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train a single default NaiveBayesModel\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/utils/decorators.py:30\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     29\u001b[0m     gargs, gkwargs \u001b[38;5;241m=\u001b[39m g(\u001b[38;5;241m*\u001b[39mother_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/predictor/predictor.py:866\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[1;32m    864\u001b[0m     aux_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit_weighted_ensemble\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[0;32m--> 866\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtuning_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlabeled_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mholdout_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mholdout_frac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bag_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_bag_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bag_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_bag_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mnum_stack_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bag_holdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_bag_holdout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_post_fit_vars()\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_fit(\n\u001b[1;32m    875\u001b[0m     keep_only_best\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeep_only_best\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    876\u001b[0m     refit_full\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrefit_full\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    880\u001b[0m     infer_limit\u001b[38;5;241m=\u001b[39minfer_limit,\n\u001b[1;32m    881\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/learner/abstract_learner.py:125\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit\u001b[0;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLearner is already fit.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_input(X\u001b[38;5;241m=\u001b[39mX, X_val\u001b[38;5;241m=\u001b[39mX_val, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/learner/default_learner.py:118\u001b[0m, in \u001b[0;36mDefaultLearner._fit\u001b[0;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_metric \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39meval_metric\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m--> 118\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mholdout_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mholdout_frac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit_trainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrainer_fit_kwargs\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_trainer(trainer\u001b[38;5;241m=\u001b[39mtrainer)\n\u001b[1;32m    132\u001b[0m time_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/trainer/auto_trainer.py:98\u001b[0m, in \u001b[0;36mAutoTrainer.fit\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_bag_holdout:\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;66;03m# TODO: User could be intending to blend instead. Add support for blend stacking.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;66;03m#  This error message is necessary because when calculating out-of-fold predictions for user, we want to return them in the form given in train_data,\u001b[39;00m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;66;03m#  but if we merge train and val here, it becomes very confusing from a users perspective, especially because we reset index, making it impossible to match\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;66;03m#  the original train_data to the out-of-fold predictions from `predictor.get_oof_pred_proba()`.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_val, y_val is not None, but bagged mode was specified. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     91\u001b[0m                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf calling from `TabularPredictor.fit()`, `tuning_data` should be None.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     92\u001b[0m                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDefault bagged mode does not use tuning data / validation data. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecify the following:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     96\u001b[0m                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mpredictor.fit(..., tuning_data=tuning_data, use_bag_holdout=True)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi_and_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m                               \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                               \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mnum_stack_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                               \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                               \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                               \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:2054\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_and_ensemble\u001b[0;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[1;32m   2051\u001b[0m model_names_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_multi_levels(X, y, hyperparameters\u001b[38;5;241m=\u001b[39mhyperparameters, X_val\u001b[38;5;241m=\u001b[39mX_val, y_val\u001b[38;5;241m=\u001b[39my_val,\n\u001b[1;32m   2052\u001b[0m                                           X_unlabeled\u001b[38;5;241m=\u001b[39mX_unlabeled, level_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, level_end\u001b[38;5;241m=\u001b[39mnum_stack_levels\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, time_limit\u001b[38;5;241m=\u001b[39mtime_limit, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_names()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2054\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoGluon did not successfully train any models\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_names_fit\n",
      "\u001b[0;31mValueError\u001b[0m: AutoGluon did not successfully train any models"
     ]
    }
   ],
   "source": [
    "custom_hyperparameters = {ConvNetModel: {}}\n",
    "# custom_hyperparameters = {NaiveBayesModel: [{}, {'var_smoothing': 0.00001}, {'var_smoothing': 0.000002}]}  # Train 3 NaiveBayes models with different hyperparameters\n",
    "predictor = TabularPredictor(label=label,eval_metric=\"precision\")\n",
    "predictor.fit(train_data=one_hot_train_data1, tuning_data=one_hot_valid_data1, hyperparameters=custom_hyperparameters,feature_generator=None, )  # Train a single default NaiveBayesModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2516aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ab3ce622",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230626_075506/\"\n",
      "Beginning AutoGluon training ... Time limit = 120s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230626_075506/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.0\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 10 13:32:12 UTC 2021\n",
      "Train Data Rows:    8281\n",
      "Train Data Columns: 13756\n",
      "Tuning Data Rows:    2943\n",
      "Tuning Data Columns: 13756\n",
      "Label Column: solubility\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting IdentityFeatureGenerator...\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "Data preprocessing and feature engineering runtime = 9.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Custom Model Type Detected: <class '__main__.ConvNetModel'>\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: ConvNetModel ... Training model for up to 110.46s of the 108.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len 13756\n",
      "Epoch 1 loss: 0.6859306526738543\n",
      "Epoch 2 loss: 0.6000387591215991\n",
      "Epoch 3 loss: 0.46730088968147604\n",
      "Epoch 4 loss: 0.3355218250622121\n",
      "Epoch 5 loss: 0.2332828393162683\n",
      "Epoch 6 loss: 0.17486146613666714\n",
      "Epoch 7 loss: 0.12697389985384175\n",
      "Epoch 8 loss: 0.11056849756097609\n",
      "Epoch 9 loss: 0.09588737711454778\n",
      "Epoch 10 loss: 0.08587926510692567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.7088\t = Validation score   (accuracy)\n",
      "\t1573.04s\t = Training   runtime\n",
      "\t11.31s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 110.46s of the -1480.53s of remaining time.\n",
      "\t0.7088\t = Validation score   (accuracy)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1601.66s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230626_075506/\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x2b50b8995520>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################\n",
    "# Training custom model using TabularPredictor #\n",
    "################################################\n",
    "\n",
    "custom_hyperparameters = {ConvNetModel: {}}\n",
    "# custom_hyperparameters = {NaiveBayesModel: [{}, {'var_smoothing': 0.00001}, {'var_smoothing': 0.000002}]}  # Train 3 NaiveBayes models with different hyperparameters\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"precision\")\n",
    "predictor.fit(train_data=one_hot_train_data1, tuning_data=one_hot_valid_data1, hyperparameters=custom_hyperparameters,feature_generator=None,)  # Train a single default NaiveBayesModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2901767f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cda43e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6326530612244898,\n",
       " 'balanced_accuracy': 0.5077271223351983,\n",
       " 'mcc': 0.03790849756936191,\n",
       " 'roc_auc': 0.5186949694075584,\n",
       " 'f1': 0.7698863636363636,\n",
       " 'precision': 0.6401574803149607,\n",
       " 'recall': 0.9655581947743468}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(one_hot_test_data, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "14bb388c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.708800543662929,\n",
       " 'balanced_accuracy': 0.5137198144802362,\n",
       " 'mcc': 0.13176302073190968,\n",
       " 'roc_auc': 0.5268012162340809,\n",
       " 'f1': 0.8278770837517574,\n",
       " 'precision': 0.7067901234567902,\n",
       " 'recall': 0.9990305380513815}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(one_hot_valid_data1, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d776bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "449fb73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NN_TORCH': {}, 'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'], 'CAT': {}, 'XGB': {}, 'FASTAI': {}, 'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}], 'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}], 'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}]}\n"
     ]
    }
   ],
   "source": [
    "from autogluon.common import space\n",
    "from autogluon.tabular.configs.hyperparameter_configs import get_hyperparameter_config\n",
    "\n",
    "custom_hyperparameters = get_hyperparameter_config('default')\n",
    "\n",
    "# custom_hyperparameters[CustomRandomForestModel] = best_model_info['hyperparameters']\n",
    "\n",
    "print(custom_hyperparameters)\n",
    "\n",
    "predictor = TabularPredictor(label=label)\n",
    "predictor.fit(train_data=one_hot_train_data1, tuning_data=one_hot_valid_data1, hyperparameters=custom_hyperparameters,feature_generator=None, eval_metric=\"precision\",time_limit=360)  # Train a single default NaiveBayesModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7980b83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230627_040057/\"\n",
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (8281 samples, 113.99 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230627_040057/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.0\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 10 13:32:12 UTC 2021\n",
      "Train Data Rows:    8281\n",
      "Train Data Columns: 13756\n",
      "Tuning Data Rows:    2943\n",
      "Tuning Data Columns: 13756\n",
      "Label Column: solubility\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting IdentityFeatureGenerator...\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "Data preprocessing and feature engineering runtime = 15.27s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Custom Model Type Detected: <class '__main__.ConvNetModel'>\n",
      "Fitting 1 L1 models ...\n",
      "Hyperparameter tuning model: ConvNetModel ...\n",
      "\tNo hyperparameter search space specified for ConvNetModel. Skipping HPO. Will train one model based on the provided hyperparameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len 13756\n",
      "Epoch 1 loss: 0.6719854345848394\n",
      "Epoch 2 loss: 0.5943438645250113\n",
      "Epoch 3 loss: 0.4359022209117579\n",
      "Epoch 4 loss: 0.28266195161629093\n",
      "Epoch 5 loss: 0.17684369366760402\n",
      "Epoch 6 loss: 0.14072937192034352\n",
      "Epoch 7 loss: 0.11817425430225309\n",
      "Epoch 8 loss: 0.10011605590278673\n",
      "Epoch 9 loss: 0.09068134251274457\n",
      "Epoch 10 loss: 0.08874948418793059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitted model: ConvNetModel ...\n",
      "\t0.6925\t = Validation score   (accuracy)\n",
      "\t1464.77s\t = Training   runtime\n",
      "\t8.43s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.6925\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1521.97s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230627_040057/\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x2b002eccd7f0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogluon.common import space\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "ConvNetOptions = {\n",
    "    \"batch_size\": 128,\n",
    "    'lr': space.Real(1e-4, 1e-2, default=5e-4, log=True),\n",
    "    \"epochs\": space.Int(lower=5, upper=10, default=5),\n",
    "}\n",
    "\n",
    "CustomHyperparameters = {ConvNetModel : ConvNetOptions}\n",
    "\n",
    "TuneKwargs = {\n",
    "    \"searcher\": \"bayes\",\n",
    "    \"scheduler\": \"local\",\n",
    "    \"num_trials\": 3,\n",
    "}\n",
    "\n",
    "predictor = TabularPredictor(label=label)\n",
    "predictor.fit(train_data=one_hot_train_data1,\n",
    "              tuning_data=one_hot_valid_data1,\n",
    "              hyperparameters=CustomHyperparameters,\n",
    "              hyperparameter_tune_kwargs=TuneKwargs,\n",
    "              feature_generator=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82783506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230627_022636/\"\n",
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (8281 samples, 113.99 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230627_022636/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.0\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 10 13:32:12 UTC 2021\n",
      "Train Data Rows:    8281\n",
      "Train Data Columns: 13756\n",
      "Tuning Data Rows:    2943\n",
      "Tuning Data Columns: 13756\n",
      "Label Column: solubility\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting IdentityFeatureGenerator...\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "Data preprocessing and feature engineering runtime = 8.39s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Custom Model Type Detected: <class '__main__.ConvNetModel'>\n",
      "Fitting 1 L1 models ...\n",
      "Hyperparameter tuning model: ConvNetModel ...\n",
      "\tNo hyperparameter search space specified for ConvNetModel. Skipping HPO. Will train one model based on the provided hyperparameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len 13756\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptySearchSpace\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py:1271\u001b[0m, in \u001b[0;36mAbstractModel._hyperparameter_tune\u001b[0;34m(self, X, y, X_val, y_val, hpo_executor, **kwargs)\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1271\u001b[0m     \u001b[43mhpo_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_search_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EmptySearchSpace:\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/hpo/executors.py:477\u001b[0m, in \u001b[0;36mCustomHpoExecutor.validate_search_space\u001b[0;34m(self, search_space, model_name)\u001b[0m\n\u001b[1;32m    475\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mNo hyperparameter search space specified for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Skipping HPO. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    476\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWill train one model based on the provided hyperparameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 477\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EmptySearchSpace\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_space \u001b[38;5;241m=\u001b[39m search_space\n",
      "\u001b[0;31mEmptySearchSpace\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m tune_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearcher\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbayes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# bayes\u001b[39;00m\n\u001b[1;32m     14\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_trials\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     16\u001b[0m               }\n\u001b[1;32m     17\u001b[0m predictor \u001b[38;5;241m=\u001b[39m TabularPredictor(label\u001b[38;5;241m=\u001b[39mlabel)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mone_hot_train_data1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtuning_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mone_hot_valid_data1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m              \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_hyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m              \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m              \u001b[49m\u001b[43mfeature_generator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train a single default NaiveBayesModel\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/utils/decorators.py:30\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     29\u001b[0m     gargs, gkwargs \u001b[38;5;241m=\u001b[39m g(\u001b[38;5;241m*\u001b[39mother_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/predictor/predictor.py:866\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[1;32m    864\u001b[0m     aux_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit_weighted_ensemble\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[0;32m--> 866\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtuning_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlabeled_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mholdout_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mholdout_frac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bag_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_bag_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bag_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_bag_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mnum_stack_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bag_holdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_bag_holdout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_post_fit_vars()\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_fit(\n\u001b[1;32m    875\u001b[0m     keep_only_best\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeep_only_best\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    876\u001b[0m     refit_full\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrefit_full\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    880\u001b[0m     infer_limit\u001b[38;5;241m=\u001b[39minfer_limit,\n\u001b[1;32m    881\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/learner/abstract_learner.py:125\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit\u001b[0;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLearner is already fit.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_input(X\u001b[38;5;241m=\u001b[39mX, X_val\u001b[38;5;241m=\u001b[39mX_val, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/learner/default_learner.py:118\u001b[0m, in \u001b[0;36mDefaultLearner._fit\u001b[0;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_metric \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39meval_metric\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m--> 118\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mholdout_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mholdout_frac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit_trainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrainer_fit_kwargs\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_trainer(trainer\u001b[38;5;241m=\u001b[39mtrainer)\n\u001b[1;32m    132\u001b[0m time_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/trainer/auto_trainer.py:98\u001b[0m, in \u001b[0;36mAutoTrainer.fit\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_bag_holdout:\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;66;03m# TODO: User could be intending to blend instead. Add support for blend stacking.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;66;03m#  This error message is necessary because when calculating out-of-fold predictions for user, we want to return them in the form given in train_data,\u001b[39;00m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;66;03m#  but if we merge train and val here, it becomes very confusing from a users perspective, especially because we reset index, making it impossible to match\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;66;03m#  the original train_data to the out-of-fold predictions from `predictor.get_oof_pred_proba()`.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_val, y_val is not None, but bagged mode was specified. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     91\u001b[0m                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf calling from `TabularPredictor.fit()`, `tuning_data` should be None.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     92\u001b[0m                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDefault bagged mode does not use tuning data / validation data. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecify the following:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     96\u001b[0m                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mpredictor.fit(..., tuning_data=tuning_data, use_bag_holdout=True)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi_and_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m                               \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                               \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mnum_stack_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                               \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                               \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                               \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:2051\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_and_ensemble\u001b[0;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_rows_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_val)\n\u001b[1;32m   2050\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_cols_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m-> 2051\u001b[0m model_names_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_multi_levels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stack_levels\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_names()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2054\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoGluon did not successfully train any models\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:312\u001b[0m, in \u001b[0;36mAbstractTrainer.train_multi_levels\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[1;32m    310\u001b[0m         core_kwargs_level[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m core_kwargs_level\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m'\u001b[39m, time_limit_core)\n\u001b[1;32m    311\u001b[0m         aux_kwargs_level[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m aux_kwargs_level\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m'\u001b[39m, time_limit_aux)\n\u001b[0;32m--> 312\u001b[0m     base_model_names, aux_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_new_level\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_kwargs_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_kwargs_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname_suffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     model_names_fit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m base_model_names \u001b[38;5;241m+\u001b[39m aux_models\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_best \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_names_fit) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:429\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level\u001b[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[1;32m    427\u001b[0m     core_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m core_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m name_suffix\n\u001b[1;32m    428\u001b[0m     aux_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m aux_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m name_suffix\n\u001b[0;32m--> 429\u001b[0m core_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_new_level_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     aux_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_new_level_aux(X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, base_model_names\u001b[38;5;241m=\u001b[39mcore_models, level\u001b[38;5;241m=\u001b[39mlevel\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    434\u001b[0m                                           infer_limit\u001b[38;5;241m=\u001b[39minfer_limit, infer_limit_batch_size\u001b[38;5;241m=\u001b[39minfer_limit_batch_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maux_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:520\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level_core\u001b[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m fit_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstack_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstack_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:2021\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi\u001b[0;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_repeat_start \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2020\u001b[0m     time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 2021\u001b[0m     model_names_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi_initial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_fold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_repeats_initial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mfeature_prune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_prune_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2023\u001b[0m     n_repeat_start \u001b[38;5;241m=\u001b[39m n_repeats_initial\n\u001b[1;32m   2024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:1913\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_initial\u001b[0;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m bagged:\n\u001b[1;32m   1912\u001b[0m     time_ratio \u001b[38;5;241m=\u001b[39m hpo_time_ratio \u001b[38;5;28;01mif\u001b[39;00m hpo_enabled \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1913\u001b[0m     models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1914\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1916\u001b[0m     time_ratio \u001b[38;5;241m=\u001b[39m hpo_time_ratio \u001b[38;5;28;01mif\u001b[39;00m hpo_enabled \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:1992\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_fold\u001b[0;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1990\u001b[0m         time_start_model \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   1991\u001b[0m         time_left \u001b[38;5;241m=\u001b[39m time_limit \u001b[38;5;241m-\u001b[39m (time_start_model \u001b[38;5;241m-\u001b[39m time_start)\n\u001b[0;32m-> 1992\u001b[0m model_name_trained_lst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_single_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_tune_kwargs_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m   1995\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:1776\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single_full\u001b[0;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, **kwargs)\u001b[0m\n\u001b[1;32m   1761\u001b[0m     hpo_models, hpo_results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mhyperparameter_tune(\n\u001b[1;32m   1762\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   1763\u001b[0m         y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1773\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_fit_kwargs\n\u001b[1;32m   1774\u001b[0m     )\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     hpo_models, hpo_results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparameter_tune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_resources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_fit_kwargs\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(hpo_models) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1786\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo model was trained during hyperparameter tuning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m... Skipping this model.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py:1257\u001b[0m, in \u001b[0;36mAbstractModel.hyperparameter_tune\u001b[0;34m(self, hyperparameter_tune_kwargs, hpo_executor, time_limit, **kwargs)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_fit_resources(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1256\u001b[0m hpo_executor\u001b[38;5;241m.\u001b[39mregister_resources(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hyperparameter_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhpo_executor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhpo_executor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py:1273\u001b[0m, in \u001b[0;36mAbstractModel._hyperparameter_tune\u001b[0;34m(self, X, y, X_val, y_val, hpo_executor, **kwargs)\u001b[0m\n\u001b[1;32m   1271\u001b[0m     hpo_executor\u001b[38;5;241m.\u001b[39mvalidate_search_space(search_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EmptySearchSpace:\n\u001b[0;32m-> 1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mskip_hpo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;66;03m# Use absolute path here because ray tune will change the working directory\u001b[39;00m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_contexts(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath) \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep)\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/models/abstract/model_trial.py:126\u001b[0m, in \u001b[0;36mskip_hpo\u001b[0;34m(model, X, y, X_val, y_val, time_limit, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m fit_model_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    125\u001b[0m predict_proba_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(X\u001b[38;5;241m=\u001b[39mX_val)\n\u001b[0;32m--> 126\u001b[0m \u001b[43mfit_and_save_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_model_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_proba_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_proba_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m hpo_results \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_time\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mfit_time}\n\u001b[1;32m    135\u001b[0m hpo_model_performances \u001b[38;5;241m=\u001b[39m {model\u001b[38;5;241m.\u001b[39mname: model\u001b[38;5;241m.\u001b[39mval_score}\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/models/abstract/model_trial.py:101\u001b[0m, in \u001b[0;36mfit_and_save_model\u001b[0;34m(model, fit_args, predict_proba_args, y_val, time_start, time_limit)\u001b[0m\n\u001b[1;32m     98\u001b[0m     time_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    100\u001b[0m time_fit_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 101\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_left\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m time_fit_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_get_tags()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_oof\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py:703\u001b[0m, in \u001b[0;36mAbstractModel.fit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_fit_resources(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_memory_usage(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 703\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[5], line 106\u001b[0m, in \u001b[0;36mConvNetModel._fit\u001b[0;34m(self, X, y, batch_size, epochs, lr, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m\"\u001b[39m,seq_len)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m ConvNet(input_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, seq_len\u001b[38;5;241m=\u001b[39mseq_len)\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 93\u001b[0m, in \u001b[0;36mConvNetModel.train\u001b[0;34m(self, model, X_train, y_train, batch_size, epochs, lr)\u001b[0m\n\u001b[1;32m     91\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     92\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), labels)\n\u001b[0;32m---> 93\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     95\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoML/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train customised model with other models together \n",
    "from autogluon.common import space\n",
    "CustomRandomForestModel\n",
    "\n",
    "ConvNetoptions =  {\n",
    "    \"batch_size\":128, \n",
    "    'lr': space.Real(1e-4, 1e-2, default=5e-4, log=True),\n",
    "    \"epochs\":space.Int(lower=5, upper=10, default=5), \n",
    "}\n",
    "\n",
    "custom_hyperparameters = {ConvNetModel: ConvNetoptions}\n",
    "\n",
    "tune_kwargs = {\n",
    "                  \"searcher\": \"bayes\", # bayes\n",
    "                  \"scheduler\": \"local\",\n",
    "                  \"num_trials\": 3,\n",
    "              }\n",
    "predictor = TabularPredictor(label=label)\n",
    "predictor.fit(train_data=one_hot_train_data1, \n",
    "              tuning_data=one_hot_valid_data1, \n",
    "              hyperparameters=custom_hyperparameters, \n",
    "              hyperparameter_tune_kwargs= \"auto\",\n",
    "              feature_generator=None, )  # Train a single default NaiveBayesModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aeefb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.evaluate(one_hot_test_data, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3dd5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.evaluate(one_hot_valid_data1, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c9f4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af66ece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230627_053839/\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NN_TORCH': {}, 'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'], 'CAT': {}, 'XGB': {}, 'FASTAI': {}, 'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}], 'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}], 'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}], <class '__main__.ConvNetModel'>: {}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (8281 samples, 113.99 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230627_053839/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.0\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 10 13:32:12 UTC 2021\n",
      "Train Data Rows:    8281\n",
      "Train Data Columns: 13756\n",
      "Tuning Data Rows:    2943\n",
      "Tuning Data Columns: 13756\n",
      "Label Column: solubility\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting IdentityFeatureGenerator...\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "Data preprocessing and feature engineering runtime = 6.9s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'precision'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Custom Model Type Detected: <class '__main__.ConvNetModel'>\n",
      "Fitting 14 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
      "Fitting model: KNeighborsDist ...\n",
      "\tNo valid features to train KNeighborsDist... Skipping this model.\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.7131\t = Validation score   (precision)\n",
      "\t26.5s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.7131\t = Validation score   (precision)\n",
      "\t25.25s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7053\t = Validation score   (precision)\n",
      "\t18.31s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.7022\t = Validation score   (precision)\n",
      "\t19.48s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.716\t = Validation score   (precision)\n",
      "\t282.82s\t = Training   runtime\n",
      "\t12.08s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.706\t = Validation score   (precision)\n",
      "\t18.6s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.701\t = Validation score   (precision)\n",
      "\t19.19s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\t0.7451\t = Validation score   (precision)\n",
      "\t2515.4s\t = Training   runtime\n",
      "\t70.73s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.7236\t = Validation score   (precision)\n",
      "\t178.14s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\tlist index out of range\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1502, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1447, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 703, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    self._train_net(train_dataset=train_dataset,\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 298, in _train_net\n",
      "    loss = self.model.compute_loss(data_batch, **loss_kwargs)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/torch_network_modules.py\", line 231, in compute_loss\n",
      "    predict_data = self(data_batch)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/torch_network_modules.py\", line 169, in forward\n",
      "    input_data = input_data[0]\n",
      "IndexError: list index out of range\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.7114\t = Validation score   (precision)\n",
      "\t37.44s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Fitting model: ConvNetModel ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len 13756\n",
      "Epoch 1 loss: 0.6715952516064163\n",
      "Epoch 2 loss: 0.5869934253914412\n",
      "Epoch 3 loss: 0.4224137973415759\n",
      "Epoch 4 loss: 0.26641654200101084\n",
      "Epoch 5 loss: 0.1738036938387063\n",
      "Epoch 6 loss: 0.13570972154451202\n",
      "Epoch 7 loss: 0.11850019416576901\n",
      "Epoch 8 loss: 0.09886133019603038\n",
      "Epoch 9 loss: 0.08607578083020888\n",
      "Epoch 10 loss: 0.08309740766290893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.7136\t = Validation score   (precision)\n",
      "\t754.92s\t = Training   runtime\n",
      "\t6.55s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.7452\t = Validation score   (precision)\n",
      "\t2.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4038.58s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230627_053839/\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x2b002cee0340>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogluon.tabular.configs.hyperparameter_configs import get_hyperparameter_config\n",
    "\n",
    "# Now we can add the custom model with tuned hyperparameters to be trained alongside the default models:\n",
    "custom_hyperparameters = get_hyperparameter_config('default')\n",
    "\n",
    "custom_hyperparameters[ConvNetModel] = {}\n",
    "\n",
    "print(custom_hyperparameters)\n",
    "\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"precision\")\n",
    "predictor.fit(train_data=one_hot_train_data1, tuning_data=one_hot_valid_data1, hyperparameters=custom_hyperparameters,feature_generator=None)  # Train a single default NaiveBayesModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ed349af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6659663865546218,\n",
       " 'accuracy': 0.6024187452758881,\n",
       " 'balanced_accuracy': 0.5459232300087407,\n",
       " 'mcc': 0.09835253929687982,\n",
       " 'roc_auc': 0.5585503281465277,\n",
       " 'f1': 0.7068004459308808,\n",
       " 'recall': 0.7529691211401425}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(one_hot_test_data, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "104e3f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7452443118239462,\n",
       " 'accuracy': 0.745837580699966,\n",
       " 'balanced_accuracy': 0.5961780615167673,\n",
       " 'mcc': 0.3092416007181366,\n",
       " 'roc_auc': 0.6757819041113999,\n",
       " 'f1': 0.8423271500843171,\n",
       " 'recall': 0.9684924866698982}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(one_hot_valid_data1, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91289560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NeuralNetFastAI</td>\n",
       "      <td>0.666316</td>\n",
       "      <td>0.745054</td>\n",
       "      <td>40.648630</td>\n",
       "      <td>70.726684</td>\n",
       "      <td>2515.404765</td>\n",
       "      <td>40.648630</td>\n",
       "      <td>70.726684</td>\n",
       "      <td>2515.404765</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.665966</td>\n",
       "      <td>0.745244</td>\n",
       "      <td>40.922761</td>\n",
       "      <td>71.059309</td>\n",
       "      <td>2695.919962</td>\n",
       "      <td>0.011630</td>\n",
       "      <td>0.004929</td>\n",
       "      <td>2.373953</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.651310</td>\n",
       "      <td>0.723614</td>\n",
       "      <td>0.262501</td>\n",
       "      <td>0.327696</td>\n",
       "      <td>178.141245</td>\n",
       "      <td>0.262501</td>\n",
       "      <td>0.327696</td>\n",
       "      <td>178.141245</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.646192</td>\n",
       "      <td>0.715955</td>\n",
       "      <td>8.912420</td>\n",
       "      <td>12.076163</td>\n",
       "      <td>282.817101</td>\n",
       "      <td>8.912420</td>\n",
       "      <td>12.076163</td>\n",
       "      <td>282.817101</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.645032</td>\n",
       "      <td>0.713106</td>\n",
       "      <td>0.245141</td>\n",
       "      <td>0.421560</td>\n",
       "      <td>25.254385</td>\n",
       "      <td>0.245141</td>\n",
       "      <td>0.421560</td>\n",
       "      <td>25.254385</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LightGBMXT</td>\n",
       "      <td>0.645032</td>\n",
       "      <td>0.713106</td>\n",
       "      <td>0.375744</td>\n",
       "      <td>0.474071</td>\n",
       "      <td>26.496801</td>\n",
       "      <td>0.375744</td>\n",
       "      <td>0.474071</td>\n",
       "      <td>26.496801</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ConvNetModel</td>\n",
       "      <td>0.642646</td>\n",
       "      <td>0.713561</td>\n",
       "      <td>5.233721</td>\n",
       "      <td>6.548399</td>\n",
       "      <td>754.924919</td>\n",
       "      <td>5.233721</td>\n",
       "      <td>6.548399</td>\n",
       "      <td>754.924919</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LightGBMLarge</td>\n",
       "      <td>0.641252</td>\n",
       "      <td>0.711378</td>\n",
       "      <td>0.338435</td>\n",
       "      <td>0.466935</td>\n",
       "      <td>37.443775</td>\n",
       "      <td>0.338435</td>\n",
       "      <td>0.466935</td>\n",
       "      <td>37.443775</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ExtraTreesGini</td>\n",
       "      <td>0.640398</td>\n",
       "      <td>0.706023</td>\n",
       "      <td>0.695148</td>\n",
       "      <td>0.203592</td>\n",
       "      <td>18.604183</td>\n",
       "      <td>0.695148</td>\n",
       "      <td>0.203592</td>\n",
       "      <td>18.604183</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RandomForestGini</td>\n",
       "      <td>0.639080</td>\n",
       "      <td>0.705299</td>\n",
       "      <td>0.612180</td>\n",
       "      <td>0.193828</td>\n",
       "      <td>18.308598</td>\n",
       "      <td>0.612180</td>\n",
       "      <td>0.193828</td>\n",
       "      <td>18.308598</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RandomForestEntr</td>\n",
       "      <td>0.637396</td>\n",
       "      <td>0.702178</td>\n",
       "      <td>0.733190</td>\n",
       "      <td>0.213623</td>\n",
       "      <td>19.475595</td>\n",
       "      <td>0.733190</td>\n",
       "      <td>0.213623</td>\n",
       "      <td>19.475595</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ExtraTreesEntr</td>\n",
       "      <td>0.636157</td>\n",
       "      <td>0.700985</td>\n",
       "      <td>0.612939</td>\n",
       "      <td>0.277064</td>\n",
       "      <td>19.193262</td>\n",
       "      <td>0.612939</td>\n",
       "      <td>0.277064</td>\n",
       "      <td>19.193262</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  score_test  score_val  pred_time_test  pred_time_val  \\\n",
       "0       NeuralNetFastAI    0.666316   0.745054       40.648630      70.726684   \n",
       "1   WeightedEnsemble_L2    0.665966   0.745244       40.922761      71.059309   \n",
       "2               XGBoost    0.651310   0.723614        0.262501       0.327696   \n",
       "3              CatBoost    0.646192   0.715955        8.912420      12.076163   \n",
       "4              LightGBM    0.645032   0.713106        0.245141       0.421560   \n",
       "5            LightGBMXT    0.645032   0.713106        0.375744       0.474071   \n",
       "6          ConvNetModel    0.642646   0.713561        5.233721       6.548399   \n",
       "7         LightGBMLarge    0.641252   0.711378        0.338435       0.466935   \n",
       "8        ExtraTreesGini    0.640398   0.706023        0.695148       0.203592   \n",
       "9      RandomForestGini    0.639080   0.705299        0.612180       0.193828   \n",
       "10     RandomForestEntr    0.637396   0.702178        0.733190       0.213623   \n",
       "11       ExtraTreesEntr    0.636157   0.700985        0.612939       0.277064   \n",
       "\n",
       "       fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
       "0   2515.404765                40.648630               70.726684   \n",
       "1   2695.919962                 0.011630                0.004929   \n",
       "2    178.141245                 0.262501                0.327696   \n",
       "3    282.817101                 8.912420               12.076163   \n",
       "4     25.254385                 0.245141                0.421560   \n",
       "5     26.496801                 0.375744                0.474071   \n",
       "6    754.924919                 5.233721                6.548399   \n",
       "7     37.443775                 0.338435                0.466935   \n",
       "8     18.604183                 0.695148                0.203592   \n",
       "9     18.308598                 0.612180                0.193828   \n",
       "10    19.475595                 0.733190                0.213623   \n",
       "11    19.193262                 0.612939                0.277064   \n",
       "\n",
       "    fit_time_marginal  stack_level  can_infer  fit_order  \n",
       "0         2515.404765            1       True          8  \n",
       "1            2.373953            2       True         12  \n",
       "2          178.141245            1       True          9  \n",
       "3          282.817101            1       True          5  \n",
       "4           25.254385            1       True          2  \n",
       "5           26.496801            1       True          1  \n",
       "6          754.924919            1       True         11  \n",
       "7           37.443775            1       True         10  \n",
       "8           18.604183            1       True          6  \n",
       "9           18.308598            1       True          3  \n",
       "10          19.475595            1       True          4  \n",
       "11          19.193262            1       True          7  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard(one_hot_test_data, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc69349d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b2b1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
