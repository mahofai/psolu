No path specified. Models will be saved in: "AutogluonModels/ag-20230815_035352/"
save path: /user/mahaohui/autoML/git/psolu/AutogluonModels/ag-20230815_035352
AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).
	2 unique label values:  [1, 0]
	If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])
column_types: OrderedDict([('sid', 'categorical'), ('solubility', 'categorical'), ('seq', 'text')])
image columns: []
self._column_types :
self._hyperparameters1 :{'optimization.learning_rate': <ray.tune.search.sample.Float object at 0x2b8c612f1e50>, 'optimization.lr_decay': <ray.tune.search.sample.Float object at 0x2b8c612f1ca0>, 'optimization.weight_decay': <ray.tune.search.sample.Float object at 0x2b8c612f1850>, 'env.batch_size': <ray.tune.search.sample.Categorical object at 0x2b8c6164d0d0>, 'optimization.optim_type': <ray.tune.search.sample.Categorical object at 0x2b8c6164d160>, 'model.hf_text.checkpoint_name': 'facebook/esm2_t6_8M_UR50D', 'optimization.max_epochs': 5, 'env.num_gpus': 1}
hpo hyperparameters:{'model.names': ['categorical_mlp', 'numerical_mlp', 'timm_image', 'hf_text', 'document_transformer', 'fusion_mlp'], 'model.hf_text.checkpoint_name': 'facebook/esm2_t6_8M_UR50D', 'model.timm_image.checkpoint_name': 'swin_base_patch4_window7_224', 'model.document_transformer.checkpoint_name': 'microsoft/layoutlmv3-base', 'optimization.learning_rate': <ray.tune.search.sample.Float object at 0x2b8c612f1e50>, 'optimization.lr_decay': <ray.tune.search.sample.Float object at 0x2b8c612f1ca0>, 'optimization.weight_decay': <ray.tune.search.sample.Float object at 0x2b8c612f1850>, 'env.batch_size': <ray.tune.search.sample.Categorical object at 0x2b8c6164d0d0>, 'optimization.optim_type': <ray.tune.search.sample.Categorical object at 0x2b8c6164d160>, 'optimization.max_epochs': 5, 'env.num_gpus': 1}
hpo filter_hyperparameters:{'model.names': ['categorical_mlp', 'hf_text', 'fusion_mlp'], 'model.hf_text.checkpoint_name': 'facebook/esm2_t6_8M_UR50D', 'optimization.learning_rate': <ray.tune.search.sample.Float object at 0x2b8c612f1e50>, 'optimization.lr_decay': <ray.tune.search.sample.Float object at 0x2b8c612f1ca0>, 'optimization.weight_decay': <ray.tune.search.sample.Float object at 0x2b8c612f1850>, 'env.batch_size': <ray.tune.search.sample.Categorical object at 0x2b8c6164d0d0>, 'optimization.optim_type': <ray.tune.search.sample.Categorical object at 0x2b8c6164d160>, 'optimization.max_epochs': 5, 'env.num_gpus': 1}
hpo hyperparameter_tune_kwargs:{hyperparameter_tune_kwargs}
Resources info for NonParallelGpuResourceCalculator: {'resources_per_job': {'cpu': 104, 'gpu': 1}, 'num_parallel_jobs': 1.0, 'batches': 2, 'cpu_per_job': 104, 'gpu_per_job': 1}
resources_per_trial to be dispatched by ray tune: {'cpu': 104, 'gpu': 1}
/user/mahaohui/miniconda3/envs/automl/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py:612: DeprecationWarning: `checkpoint_dir` in `func(config, checkpoint_dir)` is being deprecated. To save and load checkpoint in trainable functions, please use the `ray.air.session` API:

from ray.air import session

def train(config):
    # ...
    session.report({"metric": metric}, checkpoint=checkpoint)

For more information please see https://docs.ray.io/en/master/tune/api_docs/trainable.html

  warnings.warn(
Removing non-optimal trials and only keep the best one.
loading file vocab.txt
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
output_shape: 2
initializing /user/mahaohui/autoML/git/psolu/AutogluonModels/ag-20230815_035352/2611d522/hf_text
loading configuration file /user/mahaohui/autoML/git/psolu/AutogluonModels/ag-20230815_035352/2611d522/hf_text/config.json
Model config EsmConfig {
  "_name_or_path": "/user/mahaohui/autoML/git/psolu/AutogluonModels/ag-20230815_035352/2611d522/hf_text",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 320,
  "initializer_range": 0.02,
  "intermediate_size": 1280,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading file vocab.txt
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
When calling assign_layer_ids(), it catches exception: parameter name: model.encoder.emb_layer_norm_after.weight belong to neither pre or post encoder names. All the layers will use the same layer_id.
outer layers are treated as head: ['model.embeddings.word_embeddings.weight', 'model.embeddings.position_embeddings.weight', 'model.encoder.layer.0.attention.self.query.weight', 'model.encoder.layer.0.attention.self.query.bias', 'model.encoder.layer.0.attention.self.key.weight', 'model.encoder.layer.0.attention.self.key.bias', 'model.encoder.layer.0.attention.self.value.weight', 'model.encoder.layer.0.attention.self.value.bias', 'model.encoder.layer.0.attention.output.dense.weight', 'model.encoder.layer.0.attention.output.dense.bias', 'model.encoder.layer.0.attention.LayerNorm.weight', 'model.encoder.layer.0.attention.LayerNorm.bias', 'model.encoder.layer.0.intermediate.dense.weight', 'model.encoder.layer.0.intermediate.dense.bias', 'model.encoder.layer.0.output.dense.weight', 'model.encoder.layer.0.output.dense.bias', 'model.encoder.layer.0.LayerNorm.weight', 'model.encoder.layer.0.LayerNorm.bias', 'model.encoder.layer.1.attention.self.query.weight', 'model.encoder.layer.1.attention.self.query.bias', 'model.encoder.layer.1.attention.self.key.weight', 'model.encoder.layer.1.attention.self.key.bias', 'model.encoder.layer.1.attention.self.value.weight', 'model.encoder.layer.1.attention.self.value.bias', 'model.encoder.layer.1.attention.output.dense.weight', 'model.encoder.layer.1.attention.output.dense.bias', 'model.encoder.layer.1.attention.LayerNorm.weight', 'model.encoder.layer.1.attention.LayerNorm.bias', 'model.encoder.layer.1.intermediate.dense.weight', 'model.encoder.layer.1.intermediate.dense.bias', 'model.encoder.layer.1.output.dense.weight', 'model.encoder.layer.1.output.dense.bias', 'model.encoder.layer.1.LayerNorm.weight', 'model.encoder.layer.1.LayerNorm.bias', 'model.encoder.layer.2.attention.self.query.weight', 'model.encoder.layer.2.attention.self.query.bias', 'model.encoder.layer.2.attention.self.key.weight', 'model.encoder.layer.2.attention.self.key.bias', 'model.encoder.layer.2.attention.self.value.weight', 'model.encoder.layer.2.attention.self.value.bias', 'model.encoder.layer.2.attention.output.dense.weight', 'model.encoder.layer.2.attention.output.dense.bias', 'model.encoder.layer.2.attention.LayerNorm.weight', 'model.encoder.layer.2.attention.LayerNorm.bias', 'model.encoder.layer.2.intermediate.dense.weight', 'model.encoder.layer.2.intermediate.dense.bias', 'model.encoder.layer.2.output.dense.weight', 'model.encoder.layer.2.output.dense.bias', 'model.encoder.layer.2.LayerNorm.weight', 'model.encoder.layer.2.LayerNorm.bias', 'model.encoder.layer.3.attention.self.query.weight', 'model.encoder.layer.3.attention.self.query.bias', 'model.encoder.layer.3.attention.self.key.weight', 'model.encoder.layer.3.attention.self.key.bias', 'model.encoder.layer.3.attention.self.value.weight', 'model.encoder.layer.3.attention.self.value.bias', 'model.encoder.layer.3.attention.output.dense.weight', 'model.encoder.layer.3.attention.output.dense.bias', 'model.encoder.layer.3.attention.LayerNorm.weight', 'model.encoder.layer.3.attention.LayerNorm.bias', 'model.encoder.layer.3.intermediate.dense.weight', 'model.encoder.layer.3.intermediate.dense.bias', 'model.encoder.layer.3.output.dense.weight', 'model.encoder.layer.3.output.dense.bias', 'model.encoder.layer.3.LayerNorm.weight', 'model.encoder.layer.3.LayerNorm.bias', 'model.encoder.layer.4.attention.self.query.weight', 'model.encoder.layer.4.attention.self.query.bias', 'model.encoder.layer.4.attention.self.key.weight', 'model.encoder.layer.4.attention.self.key.bias', 'model.encoder.layer.4.attention.self.value.weight', 'model.encoder.layer.4.attention.self.value.bias', 'model.encoder.layer.4.attention.output.dense.weight', 'model.encoder.layer.4.attention.output.dense.bias', 'model.encoder.layer.4.attention.LayerNorm.weight', 'model.encoder.layer.4.attention.LayerNorm.bias', 'model.encoder.layer.4.intermediate.dense.weight', 'model.encoder.layer.4.intermediate.dense.bias', 'model.encoder.layer.4.output.dense.weight', 'model.encoder.layer.4.output.dense.bias', 'model.encoder.layer.4.LayerNorm.weight', 'model.encoder.layer.4.LayerNorm.bias', 'model.encoder.layer.5.attention.self.query.weight', 'model.encoder.layer.5.attention.self.query.bias', 'model.encoder.layer.5.attention.self.key.weight', 'model.encoder.layer.5.attention.self.key.bias', 'model.encoder.layer.5.attention.self.value.weight', 'model.encoder.layer.5.attention.self.value.bias', 'model.encoder.layer.5.attention.output.dense.weight', 'model.encoder.layer.5.attention.output.dense.bias', 'model.encoder.layer.5.attention.LayerNorm.weight', 'model.encoder.layer.5.attention.LayerNorm.bias', 'model.encoder.layer.5.intermediate.dense.weight', 'model.encoder.layer.5.intermediate.dense.bias', 'model.encoder.layer.5.output.dense.weight', 'model.encoder.layer.5.output.dense.bias', 'model.encoder.layer.5.LayerNorm.weight', 'model.encoder.layer.5.LayerNorm.bias', 'model.encoder.emb_layer_norm_after.weight', 'model.encoder.emb_layer_norm_after.bias', 'model.pooler.dense.weight', 'model.pooler.dense.bias', 'model.contact_head.regression.weight', 'model.contact_head.regression.bias', 'head.weight', 'head.bias']
Start to fuse 3 checkpoints via the greedy soup algorithm.
/user/mahaohui/miniconda3/envs/automl/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
