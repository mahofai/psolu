{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68bf7f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example script for defining and using FeatureGenerators in AutoGluon Tabular.\n",
    "FeatureGenerators act to clean and prepare the data to maximize predictive accuracy in downstream models.\n",
    "FeatureGenerators are stateful data preprocessors which take input data (pandas DataFrame) and output transformed data (pandas DataFrame).\n",
    "FeatureGenerators are first fit on training data through the .fit_transform() function, and then transform new data through the .transform() function.\n",
    "These generators can do anything from filling NaN values (FillNaFeatureGenerator), dropping duplicate features (DropDuplicatesFeatureGenerator), generating ngram features from text (TextNgramFeatureGenerator), and much more.\n",
    "In AutoGluon's TabularPredictor, the input data is transformed via a FeatureGenerator before entering a machine learning model. Some models use this transformed input directly and others perform further transformations before making predictions.\n",
    "\n",
    "This example is intended for advanced users that have a strong understanding of feature engineering and data preparation.\n",
    "Most users can get strong performance without specifying custom feature generators due to the generic and powerful default feature generator used by AutoGluon.\n",
    "An advanced user may wish to create a custom feature generator to:\n",
    "    1. Experiment with different preprocessing pipelines to improve model quality.\n",
    "    2. Have full control over what data is being sent to downstream models.\n",
    "    3. Migrate existing pipelines into AutoGluon for ease of use and deployment.\n",
    "    4. Contribute new feature generators to AutoGluon.\n",
    "\"\"\"\n",
    "\n",
    "################\n",
    "# Loading Data #\n",
    "################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68f8581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.core.utils.loaders import load_pd\n",
    "train_data  = load_pd.load('./train.csv')\n",
    "test_data  = load_pd.load('./test.csv')\n",
    "train_data.head()\n",
    "label = 'solubility'\n",
    "splitted_valid_data = train_data[train_data[\"fold\"] ==0.0]\n",
    "splitted_train_data = train_data[train_data[\"fold\"] !=0.0]\n",
    "# valid_data = valid_data[[\"seq\",\"solubility\"]]\n",
    "# train_data = train_data[[\"seq\",\"solubility\"]]\n",
    "# # train_data = train_data[:1000]\n",
    "# X = train_data[[\"seq\"]]\n",
    "# y = train_data[label]\n",
    "# X_test = test_data[[\"seq\",\"solubility\"]]\n",
    "# y_test = test_data[label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59d39b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11224\n",
      "1323\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "737ce13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = pd.concat([train_data,test_data], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e95c781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               sid  solubility  \\\n",
      "0     AaCD00331182           1   \n",
      "1     AaCD00331183           1   \n",
      "2     AaCD00331184           1   \n",
      "3     AaCD00331185           1   \n",
      "4     AaCD00331621           1   \n",
      "...            ...         ...   \n",
      "1318          ZR72           1   \n",
      "1319          ZR74           1   \n",
      "1320          ZR75           1   \n",
      "1321          ZR78           1   \n",
      "1322          ZR93           1   \n",
      "\n",
      "                                                    seq  fold  \n",
      "0     MTYKDGTYSSDGTYTSPNGLETVGVELTLAADKVSAVNITVHPSNP...   0.0  \n",
      "1     MTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPADRKKT...   1.0  \n",
      "2     MKAEGNTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPA...   1.0  \n",
      "3     MQSFNSGSTKIHNAFPEESTRPQKAEGNTAMNILVLGSDSRGSSDA...   1.0  \n",
      "4     MNAPVKFEYFKNPKNRELTAVELEAFAKELDQIKQEVLDDIGEKDA...   2.0  \n",
      "...                                                 ...   ...  \n",
      "1318  MGGYKGIKADGGKVNQAKQLAAKIAKDIEACQKQTQQLAEYIEGSD...   NaN  \n",
      "1319  MAFTLSAIQQAHQQFTGVDFPKLFKAFKDMGMTYNIVNIQDGTATY...   NaN  \n",
      "1320  MASKYGINDIVEMKKQHACGTNRFKIIRMGADIRIKCENCQRSIMI...   NaN  \n",
      "1321  MNMHILYNLRTKHNLEIDELAQQLNEKYGTKYEAHQIWEWENHHHE...   NaN  \n",
      "1322  IYYRGAHYMKVTDVRLRKIQTDGRMKALVSITLDEAFVIHDLRVIE...   NaN  \n",
      "\n",
      "[12547 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# concatenated_df  = concatenated_df.fillna(value=-1)\n",
    "print(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59f8e93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sid            object\n",
       "solubility      int64\n",
       "seq            object\n",
       "fold          float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb543475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>solubility</th>\n",
       "      <th>seq</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AaCD00331182</td>\n",
       "      <td>1</td>\n",
       "      <td>MTYKDGTYSSDGTYTSPNGLETVGVELTLAADKVSAVNITVHPSNP...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AaCD00331183</td>\n",
       "      <td>1</td>\n",
       "      <td>MTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPADRKKT...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AaCD00331184</td>\n",
       "      <td>1</td>\n",
       "      <td>MKAEGNTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPA...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AaCD00331185</td>\n",
       "      <td>1</td>\n",
       "      <td>MQSFNSGSTKIHNAFPEESTRPQKAEGNTAMNILVLGSDSRGSSDA...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AaCD00331621</td>\n",
       "      <td>1</td>\n",
       "      <td>MNAPVKFEYFKNPKNRELTAVELEAFAKELDQIKQEVLDDIGEKDA...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sid  solubility  \\\n",
       "0  AaCD00331182           1   \n",
       "1  AaCD00331183           1   \n",
       "2  AaCD00331184           1   \n",
       "3  AaCD00331185           1   \n",
       "4  AaCD00331621           1   \n",
       "\n",
       "                                                 seq  fold  \n",
       "0  MTYKDGTYSSDGTYTSPNGLETVGVELTLAADKVSAVNITVHPSNP...   0.0  \n",
       "1  MTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPADRKKT...   1.0  \n",
       "2  MKAEGNTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPA...   1.0  \n",
       "3  MQSFNSGSTKIHNAFPEESTRPQKAEGNTAMNILVLGSDSRGSSDA...   1.0  \n",
       "4  MNAPVKFEYFKNPKNRELTAVELEAFAKELDQIKQEVLDDIGEKDA...   2.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96e7fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Create a custom feature generator #\n",
    "#####################################\n",
    "\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "from autogluon.features.generators import AbstractFeatureGenerator\n",
    "from autogluon.common.features.types import R_INT,R_FLOAT,R_OBJECT,R_CATEGORY,S_TEXT_AS_CATEGORY \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Feature generator to add k to all values of integer features.\n",
    "class PlusKFeatureGenerator(AbstractFeatureGenerator):\n",
    "    def __init__(self, k, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.k = k\n",
    "\n",
    "    def _fit_transform(self, X: DataFrame, **kwargs) -> (DataFrame, dict):\n",
    "        # Here we can specify any logic we want to make a stateful feature generator based on the data.\n",
    "        # Just call _transform since this isn't a stateful feature generator.\n",
    "        X_out = self._transform(X)\n",
    "        # return the output and the new special types of the data. For this generator, we don't add any new special types, so just return the input special types\n",
    "        return X_out, self.feature_metadata_in.type_group_map_special\n",
    "\n",
    "    def _transform(self, X: DataFrame) -> DataFrame:\n",
    "        # Here we can specify the logic taken to convert input data to output data post-fit. Here we can reference any variables created during fit if the generator is stateful.\n",
    "        # Because this feature generator is not stateful, we simply add k to all features.\n",
    "        return X + self.k\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_infer_features_in_args() -> dict:\n",
    "        default_infer_features = dict(valid_raw_types=[R_CATEGORY]) \n",
    "        print(default_infer_features)\n",
    "        return default_infer_features  # This limits input features to only integers. We can assume that the input to _fit_transform and _transform only contain the data post-applying this filter.\n",
    "\n",
    "\n",
    "    \n",
    "class net_charge_Generator(AbstractFeatureGenerator):\n",
    "    def __init__(self,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _fit_transform(self, X: DataFrame, **kwargs) -> (DataFrame, dict):\n",
    "        # Here we can specify any logic we want to make a stateful feature generator based on the data.\n",
    "        # Just call _transform since this isn't a stateful feature generator.\n",
    "        X_out = self._transform(X)\n",
    "        # return the output and the new special types of the data. For this generator, we don't add any new special types, so just return the input special types\n",
    "        return X_out, self.feature_metadata_in.type_group_map_special\n",
    "\n",
    "    def _transform(self, X: DataFrame) -> DataFrame:\n",
    "        # Here we can specify the logic taken to convert input data to output data post-fit. Here we can reference any variables created during fit if the generator is stateful.\n",
    "        # Because this feature generator is not stateful, we simply add k to all features.\n",
    "\n",
    "        def net_charge(seq):\n",
    "            # Define the pKa values of the amino acids at pH 7.4\n",
    "            pKa = {'D': 3.9, 'E': 4.3, 'H': 6.0, 'C': 8.3, 'Y': 10.1, 'K': 10.8, 'R': 12.5,\n",
    "                   'A': 0, 'G': 0, 'I': 0, 'L': 0, 'M': 0, 'F': 0, 'P': 0, 'S': 0, 'T': 0,\n",
    "                   'W': 0, 'V': 0}\n",
    "            # Count the number of each type of amino acid in the sequence\n",
    "            aa_count = {aa: seq.count(aa) for aa in pKa.keys()}\n",
    "\n",
    "            # Calculate the net charge of the sequence using the pKa values\n",
    "            net_charge = sum([-1 * aa_count[aa] * (10 ** (-pKa[aa])) for aa in ['D', 'E']]) \\\n",
    "                         + sum([aa_count[aa] * (10 ** (-pKa[aa])) for aa in ['K', 'R', 'H']]) \\\n",
    "                         + sum([aa_count[aa] for aa in ['C', 'Y', 'K', 'R']])\n",
    "            return net_charge\n",
    "    \n",
    "        print(\"X:\",X)\n",
    "        df = pd.DataFrame(columns=['net_charge'])\n",
    "        for column in X.columns:\n",
    "            df['net_charge'] = X['seq'].apply(net_charge)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_infer_features_in_args() -> dict:\n",
    "        default_infer_features = dict(valid_raw_types=[R_OBJECT]) \n",
    "        print(default_infer_features)\n",
    "        return default_infer_features  # This limits input features to only integers. We can assume that the input to _fit_transform and _transform only contain the data post-applying this filter.\n",
    "\n",
    "\n",
    "class count_charge_Generator(AbstractFeatureGenerator):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _fit_transform(self, X: DataFrame, **kwargs) -> (DataFrame, dict):\n",
    "        # Here we can specify any logic we want to make a stateful feature generator based on the data.\n",
    "        # Just call _transform since this isn't a stateful feature generator.\n",
    "        X_out = self._transform(X)\n",
    "        # return the output and the new special types of the data. For this generator, we don't add any new special types, so just return the input special types\n",
    "        return X_out, self.feature_metadata_in.type_group_map_special\n",
    "\n",
    "    def _transform(self, X: DataFrame) -> DataFrame:\n",
    "        # Here we can specify the logic taken to convert input data to output data post-fit. Here we can reference any variables created during fit if the generator is stateful.\n",
    "        # Because this feature generator is not stateful, we simply add k to all features.\n",
    "\n",
    "        def count_chargeed(seq):\n",
    "            charged = ['D','E','K','R','H']\n",
    "            polar = ['S','T','T','Q','C']\n",
    "            aromatic = ['Y']\n",
    "\n",
    "            hdrophobic = ['A','V','L','I','M','F','W']\n",
    "            neutral = ['P','G']\n",
    "\n",
    "            charged_counter = 0\n",
    "            polar_counter = 0\n",
    "            aromatic_counter = 0\n",
    "            hdrophobic_counter = 0\n",
    "            neutral_counter = 0\n",
    "\n",
    "            for c in seq:\n",
    "                if c in charged:\n",
    "                    charged_counter+=1\n",
    "                elif c in polar:\n",
    "                    polar_counter+=1\n",
    "                elif c in aromatic:\n",
    "                    aromatic_counter+=1\n",
    "                elif c in hdrophobic:\n",
    "                    hdrophobic_counter+=1\n",
    "                elif c in neutral:\n",
    "                    neutral_counter+=1\n",
    "            return (charged_counter,polar_counter,aromatic_counter,hdrophobic_counter,neutral_counter)\n",
    "\n",
    "        df = pd.DataFrame(columns=['charged', 'polar', 'aromatic', 'hdrophobic', 'neutral'])\n",
    "        for column in X.columns:\n",
    "            df[['charged', 'polar', 'aromatic', 'hdrophobic', 'neutral']] = X[column].apply(count_chargeed).tolist()\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_infer_features_in_args() -> dict:\n",
    "        default_infer_features = dict(valid_raw_types=[R_OBJECT]) \n",
    "        print(default_infer_features)\n",
    "        return default_infer_features  # This limits input features to only integers. We can assume that the input to _fit_transform and _transform only contain the data post-applying this filter.\n",
    "\n",
    "    \n",
    "class one_hot_Generator(AbstractFeatureGenerator):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _fit_transform(self, X: DataFrame, **kwargs) -> (DataFrame, dict):\n",
    "        # Here we can specify any logic we want to make a stateful feature generator based on the data.\n",
    "        # Just call _transform since this isn't a stateful feature generator.\n",
    "        X_out = self._transform(X)\n",
    "        # return the output and the new special types of the data. For this generator, we don't add any new special types, so just return the input special types\n",
    "        return X_out, self.feature_metadata_in.type_group_map_special\n",
    "\n",
    "    def _transform(self, X: DataFrame) -> DataFrame:\n",
    "        # Here we can specify the logic taken to convert input data to output data post-fit. Here we can reference any variables created during fit if the generator is stateful.\n",
    "        # Because this feature generator is not stateful, we simply add k to all features.\n",
    "        letter_to_int = {'C': 0, 'P': 1, 'R': 2, 'N': 3, 'F': 4, 'K': 5, 'A': 6, 'H': 7, 'Y': 8, 'V': 9, 'L': 10, 'D': 11, 'G': 12, 'E': 13, 'Q': 14, 'M': 15, 'T': 16, 'S': 17, 'I': 18, 'W': 19, 'X':20}\n",
    "\n",
    "        def one_hot_encoding(sequence,letter_to_int):\n",
    "            letter_sequence = [letter_to_int[letter] for letter in sequence]\n",
    "\n",
    "            encoded_tensor  = torch.zeros((len(letter_to_int),len(sequence)), dtype=torch.int64)\n",
    "            for i in range(len(letter_sequence)):\n",
    "                encoded_tensor[letter_sequence[i],i] = 1\n",
    "            return encoded_tensor\n",
    "        \n",
    "        # Convert the protein sequences to one-hot encoding\n",
    "        \n",
    "        one_hot_df = pd.DataFrame()\n",
    "\n",
    "        # get the first column \n",
    "        column = X.iloc[:, 0]\n",
    "        sequences = column.tolist()\n",
    "        one_hot_seqs = []\n",
    "        for seq in sequences:\n",
    "            one_hot_seq = one_hot_encoding(seq,letter_to_int)\n",
    "            one_hot_seqs.append(one_hot_seq.flatten().numpy())\n",
    "\n",
    "        #print(\"one_hot_seqs size\",one_hot_seqs.shape)\n",
    "        # Create a dataframe with separate columns for each amino acid position\n",
    "        max_length = max(len(seq) for seq in sequences)\n",
    "        column_name = [f'aa{i}_{aa}' for aa in letter_to_int for i in range(1, max_length+1) ]\n",
    "\n",
    "\n",
    "        #print(\"one_hot_seqs shape\",one_hot_seqs.shape)\n",
    "        df = pd.DataFrame(one_hot_seqs,columns = column_name)\n",
    "        df  = df.fillna(value=0).astype(\"bool\")\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_infer_features_in_args() -> dict:\n",
    "        default_infer_features = dict(valid_raw_types=[R_OBJECT]) \n",
    "        print(default_infer_features)\n",
    "        return default_infer_features  # This limits input features to only integers. We can assume that the input to _fit_transform and _transform only contain the data post-applying this filter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f74d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_train = one_hot_Generator(verbosity=3,features_in=['seq'])\n",
    "# one_hot_train_data = one_hot_train.fit_transform(X=train_data)\n",
    "# one_hot_train_data  = one_hot_train_data.fillna(value=0, downcast='infer')\n",
    "# print(one_hot_train_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4770865f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valid_raw_types': ['object']}\n",
      "{'valid_raw_types': ['object']}\n",
      "{'valid_raw_types': ['object']}\n",
      "X:                 sid                                                seq\n",
      "0      AaCD00331182  MTYKDGTYSSDGTYTSPNGLETVGVELTLAADKVSAVNITVHPSNP...\n",
      "1      AaCD00331183  MTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPADRKKT...\n",
      "2      AaCD00331184  MKAEGNTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPA...\n",
      "3      AaCD00331185  MQSFNSGSTKIHNAFPEESTRPQKAEGNTAMNILVLGSDSRGSSDA...\n",
      "4      AaCD00331621  MNAPVKFEYFKNPKNRELTAVELEAFAKELDQIKQEVLDDIGEKDA...\n",
      "...             ...                                                ...\n",
      "12542          ZR72  MGGYKGIKADGGKVNQAKQLAAKIAKDIEACQKQTQQLAEYIEGSD...\n",
      "12543          ZR74  MAFTLSAIQQAHQQFTGVDFPKLFKAFKDMGMTYNIVNIQDGTATY...\n",
      "12544          ZR75  MASKYGINDIVEMKKQHACGTNRFKIIRMGADIRIKCENCQRSIMI...\n",
      "12545          ZR78  MNMHILYNLRTKHNLEIDELAQQLNEKYGTKYEAHQIWEWENHHHE...\n",
      "12546          ZR93  IYYRGAHYMKVTDVRLRKIQTDGRMKALVSITLDEAFVIHDLRVIE...\n",
      "\n",
      "[12547 rows x 2 columns]\n",
      "      charged  polar  aromatic  hdrophobic  neutral  net_charge  aa2_C  aa3_C  \\\n",
      "0          20     24         3          32       13   10.999071  False  False   \n",
      "1          62     51         6         120       31   30.996253  False  False   \n",
      "2          64     51         6         121       32   31.996203  False  False   \n",
      "3          69     59         6         125       35   33.996104  False  False   \n",
      "4          92     52        18         137       46   70.997347  False  False   \n",
      "...       ...    ...       ...         ...      ...         ...    ...    ...   \n",
      "1318       29     18         3          39       10   17.998693  False  False   \n",
      "1319       30     35         4          47        9   14.998445  False  False   \n",
      "1320       23     14         1          22        4   18.999271  False  False   \n",
      "1321       37      7         5          30        3   15.998651  False  False   \n",
      "1322       38     15         4          39        9   17.998041  False  False   \n",
      "\n",
      "      aa4_C  aa5_C  ...  aa576_W  aa627_W  aa635_W  aa739_W  aa745_W  aa749_W  \\\n",
      "0     False  False  ...    False    False    False    False    False    False   \n",
      "1     False  False  ...    False    False    False    False    False    False   \n",
      "2     False  False  ...    False    False    False    False    False    False   \n",
      "3     False  False  ...    False    False    False    False    False    False   \n",
      "4     False  False  ...    False    False    False    False    False    False   \n",
      "...     ...    ...  ...      ...      ...      ...      ...      ...      ...   \n",
      "1318  False  False  ...    False    False    False    False    False    False   \n",
      "1319  False  False  ...    False    False    False    False    False    False   \n",
      "1320  False  False  ...    False    False    False    False    False    False   \n",
      "1321  False  False  ...    False    False    False    False    False    False   \n",
      "1322  False  False  ...    False    False    False    False    False    False   \n",
      "\n",
      "      aa776_W  aa810_W  solubility  fold  \n",
      "0       False    False           1   0.0  \n",
      "1       False    False           1   1.0  \n",
      "2       False    False           1   1.0  \n",
      "3       False    False           1   1.0  \n",
      "4       False    False           1   2.0  \n",
      "...       ...      ...         ...   ...  \n",
      "1318    False    False           1   NaN  \n",
      "1319    False    False           1   NaN  \n",
      "1320    False    False           1   NaN  \n",
      "1321    False    False           1   NaN  \n",
      "1322    False    False           1   NaN  \n",
      "\n",
      "[12547 rows x 13764 columns]\n"
     ]
    }
   ],
   "source": [
    "from autogluon.features.generators import CategoryFeatureGenerator, AsTypeFeatureGenerator, BulkFeatureGenerator, DropUniqueFeatureGenerator, FillNaFeatureGenerator, PipelineFeatureGenerator, OneHotEncoderFeatureGenerator,IdentityFeatureGenerator\n",
    "import copy\n",
    "train_feature_generator = PipelineFeatureGenerator(\n",
    "    generators=[\n",
    "        # Stage 1: Convert feature types to be the same as during fit. Does not need to be specified.\n",
    "        # Stage 2: Fill NaN values of data. Does not need to be specified.\n",
    "        [  # Stage 3: Add 5 to all int features and convert all object features to category features. Concatenate the outputs of each.\n",
    "            count_charge_Generator(),\n",
    "            net_charge_Generator(),\n",
    "            one_hot_Generator(verbosity=3,features_in=['seq']),\n",
    "            #OneHotEncoderFeatureGenerator(),\n",
    "            #CategoryFeatureGenerator(),\n",
    "            IdentityFeatureGenerator(infer_features_in_args=dict(\n",
    "                valid_raw_types=[R_INT, R_FLOAT])),\n",
    "        ],\n",
    "        # Stage 4: Drop any features which are always the same value (useless). Does not need to be specified.\n",
    "     ],\n",
    "    verbosity=3\n",
    ")\n",
    "\n",
    "\n",
    "one_hot_all_data = train_feature_generator.fit_transform(X=concatenated_df)\n",
    "print(one_hot_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77039f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charged         int64\n",
      "polar           int64\n",
      "aromatic        int64\n",
      "hdrophobic      int64\n",
      "neutral         int64\n",
      "               ...   \n",
      "aa749_W          bool\n",
      "aa776_W          bool\n",
      "aa810_W          bool\n",
      "solubility       int8\n",
      "fold          float64\n",
      "Length: 13764, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_all_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7309bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       charged  polar  aromatic  hdrophobic  neutral  net_charge  aa2_C  \\\n",
      "0           20     24         3          32       13   10.999071  False   \n",
      "1           62     51         6         120       31   30.996253  False   \n",
      "2           64     51         6         121       32   31.996203  False   \n",
      "3           69     59         6         125       35   33.996104  False   \n",
      "4           92     52        18         137       46   70.997347  False   \n",
      "...        ...    ...       ...         ...      ...         ...    ...   \n",
      "11219       13     12         3          26       13    7.999424  False   \n",
      "11220       13     12         3          29       14    7.999424  False   \n",
      "11221       15     12         3          31       15    7.999172  False   \n",
      "11222       32     19         2          47        9   16.998998  False   \n",
      "11223       49     31         5          88       26   27.997996  False   \n",
      "\n",
      "       aa3_C  aa4_C  aa5_C  ...  aa576_W  aa627_W  aa635_W  aa739_W  aa745_W  \\\n",
      "0      False  False  False  ...    False    False    False    False    False   \n",
      "1      False  False  False  ...    False    False    False    False    False   \n",
      "2      False  False  False  ...    False    False    False    False    False   \n",
      "3      False  False  False  ...    False    False    False    False    False   \n",
      "4      False  False  False  ...    False    False    False    False    False   \n",
      "...      ...    ...    ...  ...      ...      ...      ...      ...      ...   \n",
      "11219  False  False  False  ...    False    False    False    False    False   \n",
      "11220  False  False  False  ...    False    False    False    False    False   \n",
      "11221  False  False  False  ...    False    False    False    False    False   \n",
      "11222  False  False  False  ...    False    False    False    False    False   \n",
      "11223  False  False  False  ...    False    False    False    False    False   \n",
      "\n",
      "       aa749_W  aa776_W  aa810_W  solubility  fold  \n",
      "0        False    False    False           1   0.0  \n",
      "1        False    False    False           1   1.0  \n",
      "2        False    False    False           1   1.0  \n",
      "3        False    False    False           1   1.0  \n",
      "4        False    False    False           1   2.0  \n",
      "...        ...      ...      ...         ...   ...  \n",
      "11219    False    False    False           1   0.0  \n",
      "11220    False    False    False           1   0.0  \n",
      "11221    False    False    False           1   0.0  \n",
      "11222    False    False    False           1   0.0  \n",
      "11223    False    False    False           0   3.0  \n",
      "\n",
      "[11224 rows x 13764 columns]\n",
      "      charged  polar  aromatic  hdrophobic  neutral  net_charge  aa2_C  aa3_C  \\\n",
      "0          33     19         2          41       12   16.998851  False  False   \n",
      "1          27     28         2          39       11    8.998598  False  False   \n",
      "2          34     27         2          52        7   17.998040  False  False   \n",
      "3          27     14         4          31        4   16.999022  False  False   \n",
      "4          26     16         1          33        9   10.998595  False  False   \n",
      "...       ...    ...       ...         ...      ...         ...    ...    ...   \n",
      "1318       29     18         3          39       10   17.998693  False  False   \n",
      "1319       30     35         4          47        9   14.998445  False  False   \n",
      "1320       23     14         1          22        4   18.999271  False  False   \n",
      "1321       37      7         5          30        3   15.998651  False  False   \n",
      "1322       38     15         4          39        9   17.998041  False  False   \n",
      "\n",
      "      aa4_C  aa5_C  ...  aa576_W  aa627_W  aa635_W  aa739_W  aa745_W  aa749_W  \\\n",
      "0     False  False  ...    False    False    False    False    False    False   \n",
      "1     False  False  ...    False    False    False    False    False    False   \n",
      "2     False  False  ...    False    False    False    False    False    False   \n",
      "3     False  False  ...    False    False    False    False    False    False   \n",
      "4     False  False  ...    False    False    False    False    False    False   \n",
      "...     ...    ...  ...      ...      ...      ...      ...      ...      ...   \n",
      "1318  False  False  ...    False    False    False    False    False    False   \n",
      "1319  False  False  ...    False    False    False    False    False    False   \n",
      "1320  False  False  ...    False    False    False    False    False    False   \n",
      "1321  False  False  ...    False    False    False    False    False    False   \n",
      "1322  False  False  ...    False    False    False    False    False    False   \n",
      "\n",
      "      aa776_W  aa810_W  solubility  fold  \n",
      "0       False    False           0   NaN  \n",
      "1       False    False           0   NaN  \n",
      "2       False    False           0   NaN  \n",
      "3       False    False           1   NaN  \n",
      "4       False    False           1   NaN  \n",
      "...       ...      ...         ...   ...  \n",
      "1318    False    False           1   NaN  \n",
      "1319    False    False           1   NaN  \n",
      "1320    False    False           1   NaN  \n",
      "1321    False    False           1   NaN  \n",
      "1322    False    False           1   NaN  \n",
      "\n",
      "[1323 rows x 13764 columns]\n"
     ]
    }
   ],
   "source": [
    "one_hot_train_data = one_hot_all_data[:len(train_data)]\n",
    "one_hot_test_data = one_hot_all_data[len(train_data):]\n",
    "print(one_hot_train_data)\n",
    "print(one_hot_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "827e6dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_valid_data1 = one_hot_train_data[one_hot_train_data[\"fold\"] ==0.0]\n",
    "one_hot_train_data1 = one_hot_train_data[one_hot_train_data[\"fold\"] !=0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcc578e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_train_data1 = one_hot_train_data1.drop([\"fold\"],axis=1)\n",
    "# one_hot_train_data1.astype(bool)                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ee07255",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_valid_data1 = one_hot_valid_data1.drop([\"fold\"],axis=1)\n",
    "# one_hot_valid_data1.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1611718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charged       int64\n",
      "polar         int64\n",
      "aromatic      int64\n",
      "hdrophobic    int64\n",
      "neutral       int64\n",
      "              ...  \n",
      "aa745_W        bool\n",
      "aa749_W        bool\n",
      "aa776_W        bool\n",
      "aa810_W        bool\n",
      "solubility     int8\n",
      "Length: 13763, dtype: object\n",
      "charged       int64\n",
      "polar         int64\n",
      "aromatic      int64\n",
      "hdrophobic    int64\n",
      "neutral       int64\n",
      "              ...  \n",
      "aa745_W        bool\n",
      "aa749_W        bool\n",
      "aa776_W        bool\n",
      "aa810_W        bool\n",
      "solubility     int8\n",
      "Length: 13763, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_train_data1.dtypes)\n",
    "print(one_hot_valid_data1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42ca1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO THE ENCODING BEFOTRE SPLITING TRAIN VALID!!!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28c8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b14ee16a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230620_034218/\"\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (8281 samples, 114.39 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230620_034218/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.0\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 10 13:32:12 UTC 2021\n",
      "Train Data Rows:    8281\n",
      "Train Data Columns: 13762\n",
      "Tuning Data Rows:    2943\n",
      "Tuning Data Columns: 13762\n",
      "Label Column: solubility\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting IdentityFeatureGenerator...\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "Data preprocessing and feature engineering runtime = 7.35s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'precision'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.7405\t = Validation score   (precision)\n",
      "\t2.64s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.7417\t = Validation score   (precision)\n",
      "\t2.47s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.63024\tvalid_set's precision: 0.716032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.7195\t = Validation score   (precision)\n",
      "\t35.52s\t = Training   runtime\n",
      "\t2.04s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.7238\t = Validation score   (precision)\n",
      "\t28.99s\t = Training   runtime\n",
      "\t2.04s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7012\t = Validation score   (precision)\n",
      "\t18.53s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.7012\t = Validation score   (precision)\n",
      "\t17.84s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.7197\t = Validation score   (precision)\n",
      "\t299.1s\t = Training   runtime\n",
      "\t12.19s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.7048\t = Validation score   (precision)\n",
      "\t17.65s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.7012\t = Validation score   (precision)\n",
      "\t17.6s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "No improvement since epoch 2: early stopping\n",
      "\t0.7443\t = Validation score   (precision)\n",
      "\t1799.97s\t = Training   runtime\n",
      "\t67.72s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tWarning: Exception caused XGBoost to fail during training... Skipping this model.\n",
      "\t\tno supported conversion for types: (dtype('O'),)\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 376, in asformat\n",
      "    return convert_method(copy=copy)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/scipy/sparse/_coo.py\", line 403, in tocsr\n",
      "    data = np.empty_like(self.data, dtype=upcast(self.dtype))\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/scipy/sparse/_sputils.py\", line 53, in upcast\n",
      "    raise TypeError('no supported conversion for types: %r' % (args,))\n",
      "TypeError: no supported conversion for types: (dtype('O'),)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1502, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1447, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 703, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 98, in _fit\n",
      "    X = self.preprocess(X, is_train=True, max_category_levels=max_category_levels)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 313, in preprocess\n",
      "    X = self._preprocess(X, **kwargs)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 64, in _preprocess\n",
      "    X = self._ohe_generator.transform(X)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/models/xgboost/xgboost_utils.py\", line 90, in transform\n",
      "    X_list.append(csr_matrix(X[self.other_cols]))\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/scipy/sparse/_compressed.py\", line 84, in __init__\n",
      "    self._set_self(self.__class__(\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/scipy/sparse/_compressed.py\", line 33, in __init__\n",
      "    arg1 = arg1.asformat(self.format)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 378, in asformat\n",
      "    return convert_method()\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/scipy/sparse/_coo.py\", line 403, in tocsr\n",
      "    data = np.empty_like(self.data, dtype=upcast(self.dtype))\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/scipy/sparse/_sputils.py\", line 53, in upcast\n",
      "    raise TypeError('no supported conversion for types: %r' % (args,))\n",
      "TypeError: no supported conversion for types: (dtype('O'),)\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.741\t = Validation score   (precision)\n",
      "\t29.98s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.773446\tvalid_set's precision: 0.718647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.7201\t = Validation score   (precision)\n",
      "\t60.65s\t = Training   runtime\n",
      "\t2.1s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.7524\t = Validation score   (precision)\n",
      "\t2.43s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2469.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230620_034218/\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label='solubility',eval_metric=\"precision\").fit(train_data=one_hot_train_data1, tuning_data=one_hot_valid_data1, feature_generator=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a9219f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6559766763848397,\n",
       " 'accuracy': 0.6061980347694633,\n",
       " 'balanced_accuracy': 0.5328479859358719,\n",
       " 'mcc': 0.07601266744204434,\n",
       " 'roc_auc': 0.5811971298907166,\n",
       " 'f1': 0.7215392838054517,\n",
       " 'recall': 0.8016627078384798}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(one_hot_test_data, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ef4fcb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7523510971786834,\n",
       " 'accuracy': 0.7366632687733605,\n",
       " 'balanced_accuracy': 0.6062508262459789,\n",
       " 'mcc': 0.28663191681352157,\n",
       " 'roc_auc': 0.6906642466839994,\n",
       " 'f1': 0.8320693391115928,\n",
       " 'recall': 0.9306834706737761}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(one_hot_valid_data1, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0ff9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
