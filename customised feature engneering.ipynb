{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68bf7f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example script for defining and using FeatureGenerators in AutoGluon Tabular.\n",
    "FeatureGenerators act to clean and prepare the data to maximize predictive accuracy in downstream models.\n",
    "FeatureGenerators are stateful data preprocessors which take input data (pandas DataFrame) and output transformed data (pandas DataFrame).\n",
    "FeatureGenerators are first fit on training data through the .fit_transform() function, and then transform new data through the .transform() function.\n",
    "These generators can do anything from filling NaN values (FillNaFeatureGenerator), dropping duplicate features (DropDuplicatesFeatureGenerator), generating ngram features from text (TextNgramFeatureGenerator), and much more.\n",
    "In AutoGluon's TabularPredictor, the input data is transformed via a FeatureGenerator before entering a machine learning model. Some models use this transformed input directly and others perform further transformations before making predictions.\n",
    "\n",
    "This example is intended for advanced users that have a strong understanding of feature engineering and data preparation.\n",
    "Most users can get strong performance without specifying custom feature generators due to the generic and powerful default feature generator used by AutoGluon.\n",
    "An advanced user may wish to create a custom feature generator to:\n",
    "    1. Experiment with different preprocessing pipelines to improve model quality.\n",
    "    2. Have full control over what data is being sent to downstream models.\n",
    "    3. Migrate existing pipelines into AutoGluon for ease of use and deployment.\n",
    "    4. Contribute new feature generators to AutoGluon.\n",
    "\"\"\"\n",
    "\n",
    "################\n",
    "# Loading Data #\n",
    "################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import numpy as np\n",
    "\n",
    "train_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/AdultIncomeBinaryClassification/train_data.csv')  # can be local CSV file as well, returns Pandas DataFrame\n",
    "test_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/AdultIncomeBinaryClassification/test_data.csv')  # another Pandas DataFrame\n",
    "label = 'class'  # specifies which column do we want to predict\n",
    "sample_train_data = train_data.head(100)  # subsample for faster demo\n",
    "\n",
    "# Separate features and labels\n",
    "# Make sure to not include your label/target column when sending input to the feature generators, or else the label will be transformed as well.\n",
    "X = sample_train_data.drop(columns=[label])\n",
    "y = sample_train_data[label]\n",
    "\n",
    "X_test = test_data.drop(columns=[label])\n",
    "y_test = test_data[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68f8581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.core.utils.loaders import load_pd\n",
    "train_data  = load_pd.load('./train.csv')\n",
    "test_data  = load_pd.load('./test.csv')\n",
    "train_data.head()\n",
    "label = 'solubility'\n",
    "# valid_data = train_data[train_data[\"fold\"] ==0.0]\n",
    "# train_data = train_data[train_data[\"fold\"] !=0.0]\n",
    "# valid_data = valid_data[[\"seq\",\"solubility\"]]\n",
    "# train_data = train_data[[\"seq\",\"solubility\"]]\n",
    "# train_data = train_data[:1000]\n",
    "X = train_data[[\"seq\"]]\n",
    "y = train_data[label]\n",
    "X_test = test_data[[\"seq\",\"solubility\"]]\n",
    "y_test = test_data[label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d39b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11224\n",
      "1323\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "737ce13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = pd.concat([train_data,test_data], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e95c781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               sid  solubility  \\\n",
      "0     AaCD00331182           1   \n",
      "1     AaCD00331183           1   \n",
      "2     AaCD00331184           1   \n",
      "3     AaCD00331185           1   \n",
      "4     AaCD00331621           1   \n",
      "...            ...         ...   \n",
      "1318          ZR72           1   \n",
      "1319          ZR74           1   \n",
      "1320          ZR75           1   \n",
      "1321          ZR78           1   \n",
      "1322          ZR93           1   \n",
      "\n",
      "                                                    seq  fold  \n",
      "0     MTYKDGTYSSDGTYTSPNGLETVGVELTLAADKVSAVNITVHPSNP...   0.0  \n",
      "1     MTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPADRKKT...   1.0  \n",
      "2     MKAEGNTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPA...   1.0  \n",
      "3     MQSFNSGSTKIHNAFPEESTRPQKAEGNTAMNILVLGSDSRGSSDA...   1.0  \n",
      "4     MNAPVKFEYFKNPKNRELTAVELEAFAKELDQIKQEVLDDIGEKDA...   2.0  \n",
      "...                                                 ...   ...  \n",
      "1318  MGGYKGIKADGGKVNQAKQLAAKIAKDIEACQKQTQQLAEYIEGSD...   NaN  \n",
      "1319  MAFTLSAIQQAHQQFTGVDFPKLFKAFKDMGMTYNIVNIQDGTATY...   NaN  \n",
      "1320  MASKYGINDIVEMKKQHACGTNRFKIIRMGADIRIKCENCQRSIMI...   NaN  \n",
      "1321  MNMHILYNLRTKHNLEIDELAQQLNEKYGTKYEAHQIWEWENHHHE...   NaN  \n",
      "1322  IYYRGAHYMKVTDVRLRKIQTDGRMKALVSITLDEAFVIHDLRVIE...   NaN  \n",
      "\n",
      "[12547 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# concatenated_df  = concatenated_df.fillna(value=-1)\n",
    "print(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59f8e93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sid            object\n",
       "solubility      int64\n",
       "seq            object\n",
       "fold          float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb543475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>solubility</th>\n",
       "      <th>seq</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AaCD00331182</td>\n",
       "      <td>1</td>\n",
       "      <td>MTYKDGTYSSDGTYTSPNGLETVGVELTLAADKVSAVNITVHPSNP...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AaCD00331183</td>\n",
       "      <td>1</td>\n",
       "      <td>MTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPADRKKT...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AaCD00331184</td>\n",
       "      <td>1</td>\n",
       "      <td>MKAEGNTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPA...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AaCD00331185</td>\n",
       "      <td>1</td>\n",
       "      <td>MQSFNSGSTKIHNAFPEESTRPQKAEGNTAMNILVLGSDSRGSSDA...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AaCD00331621</td>\n",
       "      <td>1</td>\n",
       "      <td>MNAPVKFEYFKNPKNRELTAVELEAFAKELDQIKQEVLDDIGEKDA...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sid  solubility  \\\n",
       "0  AaCD00331182           1   \n",
       "1  AaCD00331183           1   \n",
       "2  AaCD00331184           1   \n",
       "3  AaCD00331185           1   \n",
       "4  AaCD00331621           1   \n",
       "\n",
       "                                                 seq  fold  \n",
       "0  MTYKDGTYSSDGTYTSPNGLETVGVELTLAADKVSAVNITVHPSNP...   0.0  \n",
       "1  MTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPADRKKT...   1.0  \n",
       "2  MKAEGNTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPA...   1.0  \n",
       "3  MQSFNSGSTKIHNAFPEESTRPQKAEGNTAMNILVLGSDSRGSSDA...   1.0  \n",
       "4  MNAPVKFEYFKNPKNRELTAVELEAFAKELDQIKQEVLDDIGEKDA...   2.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96e7fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Create a custom feature generator #\n",
    "#####################################\n",
    "\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "from autogluon.features.generators import AbstractFeatureGenerator\n",
    "from autogluon.common.features.types import R_INT,R_FLOAT,R_OBJECT,R_CATEGORY,S_TEXT_AS_CATEGORY \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Feature generator to add k to all values of integer features.\n",
    "class PlusKFeatureGenerator(AbstractFeatureGenerator):\n",
    "    def __init__(self, k, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.k = k\n",
    "\n",
    "    def _fit_transform(self, X: DataFrame, **kwargs) -> (DataFrame, dict):\n",
    "        # Here we can specify any logic we want to make a stateful feature generator based on the data.\n",
    "        # Just call _transform since this isn't a stateful feature generator.\n",
    "        X_out = self._transform(X)\n",
    "        # return the output and the new special types of the data. For this generator, we don't add any new special types, so just return the input special types\n",
    "        return X_out, self.feature_metadata_in.type_group_map_special\n",
    "\n",
    "    def _transform(self, X: DataFrame) -> DataFrame:\n",
    "        # Here we can specify the logic taken to convert input data to output data post-fit. Here we can reference any variables created during fit if the generator is stateful.\n",
    "        # Because this feature generator is not stateful, we simply add k to all features.\n",
    "        return X + self.k\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_infer_features_in_args() -> dict:\n",
    "        default_infer_features = dict(valid_raw_types=[R_CATEGORY]) \n",
    "        print(default_infer_features)\n",
    "        return default_infer_features  # This limits input features to only integers. We can assume that the input to _fit_transform and _transform only contain the data post-applying this filter.\n",
    "\n",
    "\n",
    "    \n",
    "class net_charge_Generator(AbstractFeatureGenerator):\n",
    "    def __init__(self,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _fit_transform(self, X: DataFrame, **kwargs) -> (DataFrame, dict):\n",
    "        # Here we can specify any logic we want to make a stateful feature generator based on the data.\n",
    "        # Just call _transform since this isn't a stateful feature generator.\n",
    "        X_out = self._transform(X)\n",
    "        # return the output and the new special types of the data. For this generator, we don't add any new special types, so just return the input special types\n",
    "        return X_out, self.feature_metadata_in.type_group_map_special\n",
    "\n",
    "    def _transform(self, X: DataFrame) -> DataFrame:\n",
    "        # Here we can specify the logic taken to convert input data to output data post-fit. Here we can reference any variables created during fit if the generator is stateful.\n",
    "        # Because this feature generator is not stateful, we simply add k to all features.\n",
    "\n",
    "        def net_charge(seq):\n",
    "            # Define the pKa values of the amino acids at pH 7.4\n",
    "            pKa = {'D': 3.9, 'E': 4.3, 'H': 6.0, 'C': 8.3, 'Y': 10.1, 'K': 10.8, 'R': 12.5,\n",
    "                   'A': 0, 'G': 0, 'I': 0, 'L': 0, 'M': 0, 'F': 0, 'P': 0, 'S': 0, 'T': 0,\n",
    "                   'W': 0, 'V': 0}\n",
    "            # Count the number of each type of amino acid in the sequence\n",
    "            aa_count = {aa: seq.count(aa) for aa in pKa.keys()}\n",
    "\n",
    "            # Calculate the net charge of the sequence using the pKa values\n",
    "            net_charge = sum([-1 * aa_count[aa] * (10 ** (-pKa[aa])) for aa in ['D', 'E']]) \\\n",
    "                         + sum([aa_count[aa] * (10 ** (-pKa[aa])) for aa in ['K', 'R', 'H']]) \\\n",
    "                         + sum([aa_count[aa] for aa in ['C', 'Y', 'K', 'R']])\n",
    "            return net_charge\n",
    "    \n",
    "        print(\"X:\",X)\n",
    "        df = pd.DataFrame(columns=['net_charge'])\n",
    "        for column in X.columns:\n",
    "            df['net_charge'] = X['seq'].apply(net_charge)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_infer_features_in_args() -> dict:\n",
    "        default_infer_features = dict(valid_raw_types=[R_OBJECT]) \n",
    "        print(default_infer_features)\n",
    "        return default_infer_features  # This limits input features to only integers. We can assume that the input to _fit_transform and _transform only contain the data post-applying this filter.\n",
    "\n",
    "\n",
    "class count_charge_Generator(AbstractFeatureGenerator):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _fit_transform(self, X: DataFrame, **kwargs) -> (DataFrame, dict):\n",
    "        # Here we can specify any logic we want to make a stateful feature generator based on the data.\n",
    "        # Just call _transform since this isn't a stateful feature generator.\n",
    "        X_out = self._transform(X)\n",
    "        # return the output and the new special types of the data. For this generator, we don't add any new special types, so just return the input special types\n",
    "        return X_out, self.feature_metadata_in.type_group_map_special\n",
    "\n",
    "    def _transform(self, X: DataFrame) -> DataFrame:\n",
    "        # Here we can specify the logic taken to convert input data to output data post-fit. Here we can reference any variables created during fit if the generator is stateful.\n",
    "        # Because this feature generator is not stateful, we simply add k to all features.\n",
    "\n",
    "        def count_chargeed(seq):\n",
    "            charged = ['D','E','K','R','H']\n",
    "            polar = ['S','T','T','Q','C']\n",
    "            aromatic = ['Y']\n",
    "\n",
    "            hdrophobic = ['A','V','L','I','M','F','W']\n",
    "            neutral = ['P','G']\n",
    "\n",
    "            charged_counter = 0\n",
    "            polar_counter = 0\n",
    "            aromatic_counter = 0\n",
    "            hdrophobic_counter = 0\n",
    "            neutral_counter = 0\n",
    "\n",
    "            for c in seq:\n",
    "                if c in charged:\n",
    "                    charged_counter+=1\n",
    "                elif c in polar:\n",
    "                    polar_counter+=1\n",
    "                elif c in aromatic:\n",
    "                    aromatic_counter+=1\n",
    "                elif c in hdrophobic:\n",
    "                    hdrophobic_counter+=1\n",
    "                elif c in neutral:\n",
    "                    neutral_counter+=1\n",
    "            return (charged_counter,polar_counter,aromatic_counter,hdrophobic_counter,neutral_counter)\n",
    "\n",
    "        df = pd.DataFrame(columns=['charged', 'polar', 'aromatic', 'hdrophobic', 'neutral'])\n",
    "        for column in X.columns:\n",
    "            df[['charged', 'polar', 'aromatic', 'hdrophobic', 'neutral']] = X[column].apply(count_chargeed).tolist()\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_infer_features_in_args() -> dict:\n",
    "        default_infer_features = dict(valid_raw_types=[R_OBJECT]) \n",
    "        print(default_infer_features)\n",
    "        return default_infer_features  # This limits input features to only integers. We can assume that the input to _fit_transform and _transform only contain the data post-applying this filter.\n",
    "\n",
    "    \n",
    "class one_hot_Generator(AbstractFeatureGenerator):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _fit_transform(self, X: DataFrame, **kwargs) -> (DataFrame, dict):\n",
    "        # Here we can specify any logic we want to make a stateful feature generator based on the data.\n",
    "        # Just call _transform since this isn't a stateful feature generator.\n",
    "        X_out = self._transform(X)\n",
    "        # return the output and the new special types of the data. For this generator, we don't add any new special types, so just return the input special types\n",
    "        return X_out, self.feature_metadata_in.type_group_map_special\n",
    "\n",
    "    def _transform(self, X: DataFrame) -> DataFrame:\n",
    "        # Here we can specify the logic taken to convert input data to output data post-fit. Here we can reference any variables created during fit if the generator is stateful.\n",
    "        # Because this feature generator is not stateful, we simply add k to all features.\n",
    "        letter_to_int = {'C': 0, 'P': 1, 'R': 2, 'N': 3, 'F': 4, 'K': 5, 'A': 6, 'H': 7, 'Y': 8, 'V': 9, 'L': 10, 'D': 11, 'G': 12, 'E': 13, 'Q': 14, 'M': 15, 'T': 16, 'S': 17, 'I': 18, 'W': 19, 'X':20}\n",
    "\n",
    "        def one_hot_encoding(sequence,letter_to_int):\n",
    "            letter_sequence = [letter_to_int[letter] for letter in sequence]\n",
    "\n",
    "            encoded_tensor  = torch.zeros((len(letter_to_int),len(sequence)), dtype=torch.int64)\n",
    "            for i in range(len(letter_sequence)):\n",
    "                encoded_tensor[letter_sequence[i],i] = 1\n",
    "            return encoded_tensor\n",
    "        \n",
    "        # Convert the protein sequences to one-hot encoding\n",
    "        \n",
    "        one_hot_df = pd.DataFrame()\n",
    "\n",
    "        # get the first column \n",
    "        column = X.iloc[:, 0]\n",
    "        sequences = column.tolist()\n",
    "        one_hot_seqs = []\n",
    "        for seq in sequences:\n",
    "            one_hot_seq = one_hot_encoding(seq,letter_to_int)\n",
    "            one_hot_seqs.append(one_hot_seq.flatten().numpy())\n",
    "\n",
    "        #print(\"one_hot_seqs size\",one_hot_seqs.shape)\n",
    "        # Create a dataframe with separate columns for each amino acid position\n",
    "        max_length = max(len(seq) for seq in sequences)\n",
    "        column_name = [f'aa{i}_{aa}' for aa in letter_to_int for i in range(1, max_length+1) ]\n",
    "\n",
    "\n",
    "        #print(\"one_hot_seqs shape\",one_hot_seqs.shape)\n",
    "        df = pd.DataFrame(one_hot_seqs,columns = column_name).astype(bool)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_infer_features_in_args() -> dict:\n",
    "        default_infer_features = dict(valid_raw_types=[R_OBJECT]) \n",
    "        print(default_infer_features)\n",
    "        return default_infer_features  # This limits input features to only integers. We can assume that the input to _fit_transform and _transform only contain the data post-applying this filter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f74d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_train = one_hot_Generator(verbosity=3,features_in=['seq'])\n",
    "# one_hot_train_data = one_hot_train.fit_transform(X=train_data)\n",
    "# one_hot_train_data  = one_hot_train_data.fillna(value=0, downcast='infer')\n",
    "# print(one_hot_train_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4770865f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valid_raw_types': ['object']}\n",
      "{'valid_raw_types': ['object']}\n",
      "X:                 sid                                                seq\n",
      "0      AaCD00331182  MTYKDGTYSSDGTYTSPNGLETVGVELTLAADKVSAVNITVHPSNP...\n",
      "1      AaCD00331183  MTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPADRKKT...\n",
      "2      AaCD00331184  MKAEGNTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPA...\n",
      "3      AaCD00331185  MQSFNSGSTKIHNAFPEESTRPQKAEGNTAMNILVLGSDSRGSSDA...\n",
      "4      AaCD00331621  MNAPVKFEYFKNPKNRELTAVELEAFAKELDQIKQEVLDDIGEKDA...\n",
      "...             ...                                                ...\n",
      "12542          ZR72  MGGYKGIKADGGKVNQAKQLAAKIAKDIEACQKQTQQLAEYIEGSD...\n",
      "12543          ZR74  MAFTLSAIQQAHQQFTGVDFPKLFKAFKDMGMTYNIVNIQDGTATY...\n",
      "12544          ZR75  MASKYGINDIVEMKKQHACGTNRFKIIRMGADIRIKCENCQRSIMI...\n",
      "12545          ZR78  MNMHILYNLRTKHNLEIDELAQQLNEKYGTKYEAHQIWEWENHHHE...\n",
      "12546          ZR93  IYYRGAHYMKVTDVRLRKIQTDGRMKALVSITLDEAFVIHDLRVIE...\n",
      "\n",
      "[12547 rows x 2 columns]\n",
      "      charged  polar  aromatic  hdrophobic  neutral  net_charge  solubility  \\\n",
      "0          20     24         3          32       13   10.999071           1   \n",
      "1          62     51         6         120       31   30.996253           1   \n",
      "2          64     51         6         121       32   31.996203           1   \n",
      "3          69     59         6         125       35   33.996104           1   \n",
      "4          92     52        18         137       46   70.997347           1   \n",
      "...       ...    ...       ...         ...      ...         ...         ...   \n",
      "1318       29     18         3          39       10   17.998693           1   \n",
      "1319       30     35         4          47        9   14.998445           1   \n",
      "1320       23     14         1          22        4   18.999271           1   \n",
      "1321       37      7         5          30        3   15.998651           1   \n",
      "1322       38     15         4          39        9   17.998041           1   \n",
      "\n",
      "      fold  \n",
      "0      0.0  \n",
      "1      1.0  \n",
      "2      1.0  \n",
      "3      1.0  \n",
      "4      2.0  \n",
      "...    ...  \n",
      "1318   NaN  \n",
      "1319   NaN  \n",
      "1320   NaN  \n",
      "1321   NaN  \n",
      "1322   NaN  \n",
      "\n",
      "[12547 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from autogluon.features.generators import CategoryFeatureGenerator, AsTypeFeatureGenerator, BulkFeatureGenerator, DropUniqueFeatureGenerator, FillNaFeatureGenerator, PipelineFeatureGenerator, OneHotEncoderFeatureGenerator,IdentityFeatureGenerator\n",
    "import copy\n",
    "train_feature_generator = PipelineFeatureGenerator(\n",
    "    generators=[\n",
    "        # Stage 1: Convert feature types to be the same as during fit. Does not need to be specified.\n",
    "        # Stage 2: Fill NaN values of data. Does not need to be specified.\n",
    "        [  # Stage 3: Add 5 to all int features and convert all object features to category features. Concatenate the outputs of each.\n",
    "            count_charge_Generator(),\n",
    "            net_charge_Generator(),\n",
    "            #one_hot_Generator(verbosity=3,features_in=['seq']),\n",
    "            #OneHotEncoderFeatureGenerator(),\n",
    "            #CategoryFeatureGenerator(),\n",
    "            IdentityFeatureGenerator(infer_features_in_args=dict(\n",
    "                valid_raw_types=[R_INT, R_FLOAT])),\n",
    "        ],\n",
    "        # Stage 4: Drop any features which are always the same value (useless). Does not need to be specified.\n",
    "     ],\n",
    "    verbosity=3\n",
    ")\n",
    "\n",
    "one_hot_all_data = train_feature_generator.fit_transform(X=concatenated_df)\n",
    "print(one_hot_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77039f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>charged</th>\n",
       "      <th>polar</th>\n",
       "      <th>aromatic</th>\n",
       "      <th>hdrophobic</th>\n",
       "      <th>neutral</th>\n",
       "      <th>net_charge</th>\n",
       "      <th>solubility</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>10.999071</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>51</td>\n",
       "      <td>6</td>\n",
       "      <td>120</td>\n",
       "      <td>31</td>\n",
       "      <td>30.996253</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>51</td>\n",
       "      <td>6</td>\n",
       "      <td>121</td>\n",
       "      <td>32</td>\n",
       "      <td>31.996203</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69</td>\n",
       "      <td>59</td>\n",
       "      <td>6</td>\n",
       "      <td>125</td>\n",
       "      <td>35</td>\n",
       "      <td>33.996104</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92</td>\n",
       "      <td>52</td>\n",
       "      <td>18</td>\n",
       "      <td>137</td>\n",
       "      <td>46</td>\n",
       "      <td>70.997347</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>82</td>\n",
       "      <td>52</td>\n",
       "      <td>15</td>\n",
       "      <td>137</td>\n",
       "      <td>36</td>\n",
       "      <td>50.997046</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>72</td>\n",
       "      <td>46</td>\n",
       "      <td>14</td>\n",
       "      <td>111</td>\n",
       "      <td>41</td>\n",
       "      <td>55.998026</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>65</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>108</td>\n",
       "      <td>32</td>\n",
       "      <td>32.997449</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>51</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>66</td>\n",
       "      <td>34</td>\n",
       "      <td>24.997565</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>57</td>\n",
       "      <td>46</td>\n",
       "      <td>6</td>\n",
       "      <td>112</td>\n",
       "      <td>29</td>\n",
       "      <td>29.996681</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   charged  polar  aromatic  hdrophobic  neutral  net_charge  solubility  fold\n",
       "0       20     24         3          32       13   10.999071           1   0.0\n",
       "1       62     51         6         120       31   30.996253           1   1.0\n",
       "2       64     51         6         121       32   31.996203           1   1.0\n",
       "3       69     59         6         125       35   33.996104           1   1.0\n",
       "4       92     52        18         137       46   70.997347           1   2.0\n",
       "5       82     52        15         137       36   50.997046           0   2.0\n",
       "6       72     46        14         111       41   55.998026           1   2.0\n",
       "7       65     38        10         108       32   32.997449           1   2.0\n",
       "8       51     23         6          66       34   24.997565           1   3.0\n",
       "9       57     46         6         112       29   29.996681           1   1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_all_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7309bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       charged  polar  aromatic  hdrophobic  neutral  net_charge  solubility  \\\n",
      "0           20     24         3          32       13   10.999071           1   \n",
      "1           62     51         6         120       31   30.996253           1   \n",
      "2           64     51         6         121       32   31.996203           1   \n",
      "3           69     59         6         125       35   33.996104           1   \n",
      "4           92     52        18         137       46   70.997347           1   \n",
      "...        ...    ...       ...         ...      ...         ...         ...   \n",
      "11219       13     12         3          26       13    7.999424           1   \n",
      "11220       13     12         3          29       14    7.999424           1   \n",
      "11221       15     12         3          31       15    7.999172           1   \n",
      "11222       32     19         2          47        9   16.998998           1   \n",
      "11223       49     31         5          88       26   27.997996           0   \n",
      "\n",
      "       fold  \n",
      "0       0.0  \n",
      "1       1.0  \n",
      "2       1.0  \n",
      "3       1.0  \n",
      "4       2.0  \n",
      "...     ...  \n",
      "11219   0.0  \n",
      "11220   0.0  \n",
      "11221   0.0  \n",
      "11222   0.0  \n",
      "11223   3.0  \n",
      "\n",
      "[11224 rows x 8 columns]\n",
      "      charged  polar  aromatic  hdrophobic  neutral  net_charge  solubility  \\\n",
      "0          33     19         2          41       12   16.998851           0   \n",
      "1          27     28         2          39       11    8.998598           0   \n",
      "2          34     27         2          52        7   17.998040           0   \n",
      "3          27     14         4          31        4   16.999022           1   \n",
      "4          26     16         1          33        9   10.998595           1   \n",
      "...       ...    ...       ...         ...      ...         ...         ...   \n",
      "1318       29     18         3          39       10   17.998693           1   \n",
      "1319       30     35         4          47        9   14.998445           1   \n",
      "1320       23     14         1          22        4   18.999271           1   \n",
      "1321       37      7         5          30        3   15.998651           1   \n",
      "1322       38     15         4          39        9   17.998041           1   \n",
      "\n",
      "      fold  \n",
      "0      NaN  \n",
      "1      NaN  \n",
      "2      NaN  \n",
      "3      NaN  \n",
      "4      NaN  \n",
      "...    ...  \n",
      "1318   NaN  \n",
      "1319   NaN  \n",
      "1320   NaN  \n",
      "1321   NaN  \n",
      "1322   NaN  \n",
      "\n",
      "[1323 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "one_hot_train_data = one_hot_all_data[:len(train_data)]\n",
    "one_hot_test_data = one_hot_all_data[len(train_data):]\n",
    "print(one_hot_train_data)\n",
    "print(one_hot_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "827e6dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_valid_data1 = one_hot_train_data[one_hot_train_data[\"fold\"] ==0.0]\n",
    "one_hot_train_data1 = one_hot_train_data[one_hot_train_data[\"fold\"] !=0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcc578e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       charged  polar  aromatic  hdrophobic  neutral  net_charge  solubility\n",
      "1           62     51         6         120       31   30.996253           1\n",
      "2           64     51         6         121       32   31.996203           1\n",
      "3           69     59         6         125       35   33.996104           1\n",
      "4           92     52        18         137       46   70.997347           1\n",
      "5           82     52        15         137       36   50.997046           0\n",
      "...        ...    ...       ...         ...      ...         ...         ...\n",
      "11213       69     38         8          90       32   41.997016           0\n",
      "11215       69     40         9          91       33   44.997016           1\n",
      "11216       72     38         8          92       33   43.996965           0\n",
      "11217       72     40         9          93       34   46.996965           0\n",
      "11223       49     31         5          88       26   27.997996           0\n",
      "\n",
      "[8281 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "one_hot_train_data1 = one_hot_train_data1.drop([\"fold\"],axis=1)\n",
    "print(one_hot_train_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ee07255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       charged  polar  aromatic  hdrophobic  neutral  net_charge  solubility\n",
      "0           20     24         3          32       13   10.999071           1\n",
      "10          14     19         9          30       18   13.999145           0\n",
      "21          31      9         2          20        4   22.999499           1\n",
      "61          19     10         4          32        7   11.998918           1\n",
      "69          15     12         2          27       14    6.999426           1\n",
      "...        ...    ...       ...         ...      ...         ...         ...\n",
      "11218       15     12         3          28       14    7.999172           1\n",
      "11219       13     12         3          26       13    7.999424           1\n",
      "11220       13     12         3          29       14    7.999424           1\n",
      "11221       15     12         3          31       15    7.999172           1\n",
      "11222       32     19         2          47        9   16.998998           1\n",
      "\n",
      "[2943 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "one_hot_valid_data1 = one_hot_valid_data1.drop([\"fold\"],axis=1)\n",
    "print(one_hot_valid_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1611718b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO THE ENCODING BEFOTRE SPLITING TRAIN VALID!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b14ee16a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230620_021521/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230620_021521/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.0\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 10 13:32:12 UTC 2021\n",
      "Train Data Rows:    8281\n",
      "Train Data Columns: 6\n",
      "Tuning Data Rows:    2943\n",
      "Tuning Data Columns: 6\n",
      "Label Column: solubility\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting IdentityFeatureGenerator...\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "Data preprocessing and feature engineering runtime = 0.01s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'precision'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.7405\t = Validation score   (precision)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.7417\t = Validation score   (precision)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.571978\tvalid_set's precision: 0.726689\n",
      "[2000]\tvalid_set's binary_logloss: 0.590763\tvalid_set's precision: 0.735772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.7385\t = Validation score   (precision)\n",
      "\t14.11s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.7258\t = Validation score   (precision)\n",
      "\t2.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7252\t = Validation score   (precision)\n",
      "\t1.35s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.7259\t = Validation score   (precision)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.7205\t = Validation score   (precision)\n",
      "\t5.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.7193\t = Validation score   (precision)\n",
      "\t0.65s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.7207\t = Validation score   (precision)\n",
      "\t0.68s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 0: early stopping\n",
      "\t0.7746\t = Validation score   (precision)\n",
      "\t12.01s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.7355\t = Validation score   (precision)\n",
      "\t8.82s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.741\t = Validation score   (precision)\n",
      "\t19.6s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 1.03235\tvalid_set's precision: 0.723849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.7257\t = Validation score   (precision)\n",
      "\t17.51s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.7748\t = Validation score   (precision)\n",
      "\t2.5s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 92.47s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230620_021521/\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label='solubility',eval_metric=\"precision\").fit(train_data=one_hot_train_data1, tuning_data=one_hot_valid_data1, feature_generator=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a9219f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6908517350157729,\n",
       " 'accuracy': 0.6379440665154951,\n",
       " 'balanced_accuracy': 0.5845292122014213,\n",
       " 'mcc': 0.1808854658241798,\n",
       " 'roc_auc': 0.6262413518945585,\n",
       " 'f1': 0.7328499721137758,\n",
       " 'recall': 0.7802850356294537}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(one_hot_test_data, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ef4fcb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7748478701825557,\n",
       " 'accuracy': 0.7594291539245668,\n",
       " 'balanced_accuracy': 0.6475771713744326,\n",
       " 'mcc': 0.3663681550930343,\n",
       " 'roc_auc': 0.7446131516326621,\n",
       " 'f1': 0.8436395759717314,\n",
       " 'recall': 0.9258361609306834}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(one_hot_valid_data1, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d58fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
