{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68bf7f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example script for defining and using FeatureGenerators in AutoGluon Tabular.\n",
    "FeatureGenerators act to clean and prepare the data to maximize predictive accuracy in downstream models.\n",
    "FeatureGenerators are stateful data preprocessors which take input data (pandas DataFrame) and output transformed data (pandas DataFrame).\n",
    "FeatureGenerators are first fit on training data through the .fit_transform() function, and then transform new data through the .transform() function.\n",
    "These generators can do anything from filling NaN values (FillNaFeatureGenerator), dropping duplicate features (DropDuplicatesFeatureGenerator), generating ngram features from text (TextNgramFeatureGenerator), and much more.\n",
    "In AutoGluon's TabularPredictor, the input data is transformed via a FeatureGenerator before entering a machine learning model. Some models use this transformed input directly and others perform further transformations before making predictions.\n",
    "\n",
    "This example is intended for advanced users that have a strong understanding of feature engineering and data preparation.\n",
    "Most users can get strong performance without specifying custom feature generators due to the generic and powerful default feature generator used by AutoGluon.\n",
    "An advanced user may wish to create a custom feature generator to:\n",
    "    1. Experiment with different preprocessing pipelines to improve model quality.\n",
    "    2. Have full control over what data is being sent to downstream models.\n",
    "    3. Migrate existing pipelines into AutoGluon for ease of use and deployment.\n",
    "    4. Contribute new feature generators to AutoGluon.\n",
    "\"\"\"\n",
    "\n",
    "################\n",
    "# Loading Data #\n",
    "################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import numpy as np\n",
    "\n",
    "# train_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/AdultIncomeBinaryClassification/train_data.csv')  # can be local CSV file as well, returns Pandas DataFrame\n",
    "# test_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/AdultIncomeBinaryClassification/test_data.csv')  # another Pandas DataFrame\n",
    "# label = 'class'  # specifies which column do we want to predict\n",
    "# sample_train_data = train_data.head(100)  # subsample for faster demo\n",
    "\n",
    "# # Separate features and labels\n",
    "# # Make sure to not include your label/target column when sending input to the feature generators, or else the label will be transformed as well.\n",
    "# X = sample_train_data.drop(columns=[label])\n",
    "# y = sample_train_data[label]\n",
    "\n",
    "# X_test = test_data.drop(columns=[label])\n",
    "# y_test = test_data[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68f8581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.core.utils.loaders import load_pd\n",
    "train_data  = load_pd.load('./train.csv')\n",
    "test_data  = load_pd.load('./test.csv')\n",
    "train_data.head()\n",
    "label = 'solubility'\n",
    "splitted_valid_data = train_data[train_data[\"fold\"] ==0.0]\n",
    "splitted_train_data = train_data[train_data[\"fold\"] !=0.0]\n",
    "# valid_data = valid_data[[\"seq\",\"solubility\"]]\n",
    "# train_data = train_data[[\"seq\",\"solubility\"]]\n",
    "# train_data = train_data[:1000]\n",
    "X = train_data[[\"seq\"]]\n",
    "y = train_data[label]\n",
    "X_test = test_data[[\"seq\",\"solubility\"]]\n",
    "y_test = test_data[label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d39b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11224\n",
      "1323\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "737ce13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = pd.concat([train_data,test_data], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e95c781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               sid  solubility  \\\n",
      "0     AaCD00331182           1   \n",
      "1     AaCD00331183           1   \n",
      "2     AaCD00331184           1   \n",
      "3     AaCD00331185           1   \n",
      "4     AaCD00331621           1   \n",
      "...            ...         ...   \n",
      "1318          ZR72           1   \n",
      "1319          ZR74           1   \n",
      "1320          ZR75           1   \n",
      "1321          ZR78           1   \n",
      "1322          ZR93           1   \n",
      "\n",
      "                                                    seq  fold  \n",
      "0     MTYKDGTYSSDGTYTSPNGLETVGVELTLAADKVSAVNITVHPSNP...   0.0  \n",
      "1     MTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPADRKKT...   1.0  \n",
      "2     MKAEGNTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPA...   1.0  \n",
      "3     MQSFNSGSTKIHNAFPEESTRPQKAEGNTAMNILVLGSDSRGSSDA...   1.0  \n",
      "4     MNAPVKFEYFKNPKNRELTAVELEAFAKELDQIKQEVLDDIGEKDA...   2.0  \n",
      "...                                                 ...   ...  \n",
      "1318  MGGYKGIKADGGKVNQAKQLAAKIAKDIEACQKQTQQLAEYIEGSD...   NaN  \n",
      "1319  MAFTLSAIQQAHQQFTGVDFPKLFKAFKDMGMTYNIVNIQDGTATY...   NaN  \n",
      "1320  MASKYGINDIVEMKKQHACGTNRFKIIRMGADIRIKCENCQRSIMI...   NaN  \n",
      "1321  MNMHILYNLRTKHNLEIDELAQQLNEKYGTKYEAHQIWEWENHHHE...   NaN  \n",
      "1322  IYYRGAHYMKVTDVRLRKIQTDGRMKALVSITLDEAFVIHDLRVIE...   NaN  \n",
      "\n",
      "[12547 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# concatenated_df  = concatenated_df.fillna(value=-1)\n",
    "print(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59f8e93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sid            object\n",
       "solubility      int64\n",
       "seq            object\n",
       "fold          float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb543475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>solubility</th>\n",
       "      <th>seq</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AaCD00331182</td>\n",
       "      <td>1</td>\n",
       "      <td>MTYKDGTYSSDGTYTSPNGLETVGVELTLAADKVSAVNITVHPSNP...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AaCD00331183</td>\n",
       "      <td>1</td>\n",
       "      <td>MTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPADRKKT...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AaCD00331184</td>\n",
       "      <td>1</td>\n",
       "      <td>MKAEGNTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPA...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AaCD00331185</td>\n",
       "      <td>1</td>\n",
       "      <td>MQSFNSGSTKIHNAFPEESTRPQKAEGNTAMNILVLGSDSRGSSDA...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AaCD00331621</td>\n",
       "      <td>1</td>\n",
       "      <td>MNAPVKFEYFKNPKNRELTAVELEAFAKELDQIKQEVLDDIGEKDA...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sid  solubility  \\\n",
       "0  AaCD00331182           1   \n",
       "1  AaCD00331183           1   \n",
       "2  AaCD00331184           1   \n",
       "3  AaCD00331185           1   \n",
       "4  AaCD00331621           1   \n",
       "\n",
       "                                                 seq  fold  \n",
       "0  MTYKDGTYSSDGTYTSPNGLETVGVELTLAADKVSAVNITVHPSNP...   0.0  \n",
       "1  MTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPADRKKT...   1.0  \n",
       "2  MKAEGNTAMNILVLGSDSRGSSDADVEANTATDQRADTLMLVHVPA...   1.0  \n",
       "3  MQSFNSGSTKIHNAFPEESTRPQKAEGNTAMNILVLGSDSRGSSDA...   1.0  \n",
       "4  MNAPVKFEYFKNPKNRELTAVELEAFAKELDQIKQEVLDDIGEKDA...   2.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "50bf2f76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 21])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "torch.Size([252])\n",
      "torch.Size([9, 21])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "torch.Size([189])\n",
      "torch.Size([14, 21])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "torch.Size([294])\n",
      "(3, 294)\n",
      "       aa1_C      aa1_P      aa1_R      aa1_N      aa1_F      aa1_K  \\\n",
      "0  tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)   \n",
      "1  tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)   \n",
      "2  tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)   \n",
      "\n",
      "       aa1_A      aa1_H      aa1_Y      aa1_V  ...     aa14_D     aa14_G  \\\n",
      "0  tensor(0)  tensor(0)  tensor(0)  tensor(0)  ...       None       None   \n",
      "1  tensor(0)  tensor(0)  tensor(0)  tensor(0)  ...       None       None   \n",
      "2  tensor(0)  tensor(0)  tensor(0)  tensor(0)  ...  tensor(1)  tensor(0)   \n",
      "\n",
      "      aa14_E     aa14_Q     aa14_M     aa14_T     aa14_S     aa14_I  \\\n",
      "0       None       None       None       None       None       None   \n",
      "1       None       None       None       None       None       None   \n",
      "2  tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)  tensor(0)   \n",
      "\n",
      "      aa14_W     aa14_X  \n",
      "0       None       None  \n",
      "1       None       None  \n",
      "2  tensor(0)  tensor(0)  \n",
      "\n",
      "[3 rows x 294 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example protein sequences\n",
    "sequences = ['MDEKRRAQHNEV', 'MDEKRRAQH', 'MDEKRRAQHNEVKD']\n",
    "\n",
    "def one_hot_encoding(sequence):\n",
    "    letter_to_int = {'C': 0, 'P': 1, 'R': 2, 'N': 3, 'F': 4, 'K': 5, 'A': 6, 'H': 7, 'Y': 8, 'V': 9, 'L': 10, 'D': 11, 'G': 12, 'E': 13, 'Q': 14, 'M': 15, 'T': 16, 'S': 17, 'I': 18, 'W': 19, 'X':20}\n",
    "    letter_sequence = [letter_to_int[letter] for letter in sequence]\n",
    "\n",
    "    encoded_tensor  = torch.zeros((len(sequence),len(letter_to_int)), dtype=torch.int64)\n",
    "    for i in range(len(letter_sequence)):\n",
    "        encoded_tensor[i,letter_sequence[i]] = 1\n",
    "    return encoded_tensor\n",
    "letter_to_int = {'C': 0, 'P': 1, 'R': 2, 'N': 3, 'F': 4, 'K': 5, 'A': 6, 'H': 7, 'Y': 8, 'V': 9, 'L': 10, 'D': 11, 'G': 12, 'E': 13, 'Q': 14, 'M': 15, 'T': 16, 'S': 17, 'I': 18, 'W': 19, 'X':20}\n",
    "\n",
    "\n",
    "# \n",
    "# Create a dictionary to map amino acids to one-hot vectors\n",
    "aa_dict = {'A': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "           'C': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "           'D': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "           'E': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "           'F': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "           'G': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "           'H': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "           'I': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "           'K': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "           'L': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "           'M': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "           'N': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "           'P': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "           'Q': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "           'R': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "           'S': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "           'T': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "           'V': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "           'W': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "           'Y': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\n",
    "\n",
    "# Convert the protein sequences to one-hot encoding\n",
    "one_hot_seqs = []\n",
    "for seq in sequences:\n",
    "    #one_hot_seq = np.array([aa_dict[aa] for aa in seq]) \n",
    "    one_hot_seq = one_hot_encoding(seq)\n",
    "    print(np.shape(one_hot_seq))\n",
    "    print(one_hot_seq)\n",
    "    \n",
    "    one_hot_seqs.append(one_hot_seq.flatten())\n",
    "    print(np.shape(one_hot_seq.flatten()))\n",
    "\n",
    "# Create a dataframe with separate columns for each amino acid position\n",
    "df = pd.DataFrame(one_hot_seqs, columns=[f'aa{i}_{aa}' for i in range(1, len(one_hot_seq)+1) for aa in letter_to_int])\n",
    "print(np.shape(df))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96e7fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Create a custom feature generator #\n",
    "#####################################\n",
    "\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "from autogluon.features.generators import AbstractFeatureGenerator\n",
    "from autogluon.common.features.types import R_INT,R_FLOAT,R_OBJECT,R_CATEGORY,S_TEXT_AS_CATEGORY \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Feature generator to add k to all values of integer features.\n",
    "class PlusKFeatureGenerator(AbstractFeatureGenerator):\n",
    "    def __init__(self, k, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.k = k\n",
    "\n",
    "    def _fit_transform(self, X: DataFrame, **kwargs) -> (DataFrame, dict):\n",
    "        # Here we can specify any logic we want to make a stateful feature generator based on the data.\n",
    "        # Just call _transform since this isn't a stateful feature generator.\n",
    "        X_out = self._transform(X)\n",
    "        # return the output and the new special types of the data. For this generator, we don't add any new special types, so just return the input special types\n",
    "        return X_out, self.feature_metadata_in.type_group_map_special\n",
    "\n",
    "    def _transform(self, X: DataFrame) -> DataFrame:\n",
    "        # Here we can specify the logic taken to convert input data to output data post-fit. Here we can reference any variables created during fit if the generator is stateful.\n",
    "        # Because this feature generator is not stateful, we simply add k to all features.\n",
    "        return X + self.k\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_infer_features_in_args() -> dict:\n",
    "        default_infer_features = dict(valid_raw_types=[R_CATEGORY]) \n",
    "        print(default_infer_features)\n",
    "        return default_infer_features  # This limits input features to only integers. We can assume that the input to _fit_transform and _transform only contain the data post-applying this filter.\n",
    "\n",
    "\n",
    "    \n",
    "class net_charge_Generator(AbstractFeatureGenerator):\n",
    "    def __init__(self,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _fit_transform(self, X: DataFrame, **kwargs) -> (DataFrame, dict):\n",
    "        # Here we can specify any logic we want to make a stateful feature generator based on the data.\n",
    "        # Just call _transform since this isn't a stateful feature generator.\n",
    "        X_out = self._transform(X)\n",
    "        # return the output and the new special types of the data. For this generator, we don't add any new special types, so just return the input special types\n",
    "        return X_out, self.feature_metadata_in.type_group_map_special\n",
    "\n",
    "    def _transform(self, X: DataFrame) -> DataFrame:\n",
    "        # Here we can specify the logic taken to convert input data to output data post-fit. Here we can reference any variables created during fit if the generator is stateful.\n",
    "        # Because this feature generator is not stateful, we simply add k to all features.\n",
    "\n",
    "        def net_charge(seq):\n",
    "            # Define the pKa values of the amino acids at pH 7.4\n",
    "            pKa = {'D': 3.9, 'E': 4.3, 'H': 6.0, 'C': 8.3, 'Y': 10.1, 'K': 10.8, 'R': 12.5,\n",
    "                   'A': 0, 'G': 0, 'I': 0, 'L': 0, 'M': 0, 'F': 0, 'P': 0, 'S': 0, 'T': 0,\n",
    "                   'W': 0, 'V': 0}\n",
    "            # Count the number of each type of amino acid in the sequence\n",
    "            aa_count = {aa: seq.count(aa) for aa in pKa.keys()}\n",
    "\n",
    "            # Calculate the net charge of the sequence using the pKa values\n",
    "            net_charge = sum([-1 * aa_count[aa] * (10 ** (-pKa[aa])) for aa in ['D', 'E']]) \\\n",
    "                         + sum([aa_count[aa] * (10 ** (-pKa[aa])) for aa in ['K', 'R', 'H']]) \\\n",
    "                         + sum([aa_count[aa] for aa in ['C', 'Y', 'K', 'R']])\n",
    "            return net_charge\n",
    "    \n",
    "        print(\"X:\",X)\n",
    "        df = pd.DataFrame(columns=['net_charge'])\n",
    "        for column in X.columns:\n",
    "            df['net_charge'] = X['seq'].apply(net_charge)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_infer_features_in_args() -> dict:\n",
    "        default_infer_features = dict(valid_raw_types=[R_OBJECT]) \n",
    "        print(default_infer_features)\n",
    "        return default_infer_features  # This limits input features to only integers. We can assume that the input to _fit_transform and _transform only contain the data post-applying this filter.\n",
    "\n",
    "\n",
    "class count_charge_Generator(AbstractFeatureGenerator):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _fit_transform(self, X: DataFrame, **kwargs) -> (DataFrame, dict):\n",
    "        # Here we can specify any logic we want to make a stateful feature generator based on the data.\n",
    "        # Just call _transform since this isn't a stateful feature generator.\n",
    "        X_out = self._transform(X)\n",
    "        # return the output and the new special types of the data. For this generator, we don't add any new special types, so just return the input special types\n",
    "        return X_out, self.feature_metadata_in.type_group_map_special\n",
    "\n",
    "    def _transform(self, X: DataFrame) -> DataFrame:\n",
    "        # Here we can specify the logic taken to convert input data to output data post-fit. Here we can reference any variables created during fit if the generator is stateful.\n",
    "        # Because this feature generator is not stateful, we simply add k to all features.\n",
    "\n",
    "        def count_chargeed(seq):\n",
    "            charged = ['D','E','K','R','H']\n",
    "            polar = ['S','T','T','Q','C']\n",
    "            aromatic = ['Y']\n",
    "\n",
    "            hdrophobic = ['A','V','L','I','M','F','W']\n",
    "            neutral = ['P','G']\n",
    "\n",
    "            charged_counter = 0\n",
    "            polar_counter = 0\n",
    "            aromatic_counter = 0\n",
    "            hdrophobic_counter = 0\n",
    "            neutral_counter = 0\n",
    "\n",
    "            for c in seq:\n",
    "                if c in charged:\n",
    "                    charged_counter+=1\n",
    "                elif c in polar:\n",
    "                    polar_counter+=1\n",
    "                elif c in aromatic:\n",
    "                    aromatic_counter+=1\n",
    "                elif c in hdrophobic:\n",
    "                    hdrophobic_counter+=1\n",
    "                elif c in neutral:\n",
    "                    neutral_counter+=1\n",
    "            return (charged_counter,polar_counter,aromatic_counter,hdrophobic_counter,neutral_counter)\n",
    "\n",
    "        df = pd.DataFrame(columns=['charged', 'polar', 'aromatic', 'hdrophobic', 'neutral'])\n",
    "        for column in X.columns:\n",
    "            df[['charged', 'polar', 'aromatic', 'hdrophobic', 'neutral']] = X[column].apply(count_chargeed).tolist()\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_infer_features_in_args() -> dict:\n",
    "        default_infer_features = dict(valid_raw_types=[R_OBJECT]) \n",
    "        print(default_infer_features)\n",
    "        return default_infer_features  # This limits input features to only integers. We can assume that the input to _fit_transform and _transform only contain the data post-applying this filter.\n",
    "\n",
    "    \n",
    "class one_hot_Generator(AbstractFeatureGenerator):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _fit_transform(self, X: DataFrame, **kwargs) -> (DataFrame, dict):\n",
    "        # Here we can specify any logic we want to make a stateful feature generator based on the data.\n",
    "        # Just call _transform since this isn't a stateful feature generator.\n",
    "        X_out = self._transform(X)\n",
    "        # return the output and the new special types of the data. For this generator, we don't add any new special types, so just return the input special types\n",
    "        return X_out, self.feature_metadata_in.type_group_map_special\n",
    "\n",
    "    def _transform(self, X: DataFrame) -> DataFrame:\n",
    "        # Here we can specify the logic taken to convert input data to output data post-fit. Here we can reference any variables created during fit if the generator is stateful.\n",
    "        # Because this feature generator is not stateful, we simply add k to all features.\n",
    "        letter_to_int = {'C': 0, 'P': 1, 'R': 2, 'N': 3, 'F': 4, 'K': 5, 'A': 6, 'H': 7, 'Y': 8, 'V': 9, 'L': 10, 'D': 11, 'G': 12, 'E': 13, 'Q': 14, 'M': 15, 'T': 16, 'S': 17, 'I': 18, 'W': 19, 'X':20}\n",
    "\n",
    "        def one_hot_encoding(sequence,letter_to_int):\n",
    "            letter_sequence = [letter_to_int[letter] for letter in sequence]\n",
    "\n",
    "            encoded_tensor  = torch.zeros((len(letter_to_int),len(sequence)), dtype=torch.int64)\n",
    "            for i in range(len(letter_sequence)):\n",
    "                encoded_tensor[letter_sequence[i],i] = 1\n",
    "            return encoded_tensor\n",
    "        \n",
    "        # Convert the protein sequences to one-hot encoding\n",
    "        \n",
    "        one_hot_df = pd.DataFrame()\n",
    "\n",
    "        # get the first column \n",
    "        column = X.iloc[:, 0]\n",
    "        sequences = column.tolist()\n",
    "        one_hot_seqs = []\n",
    "        for seq in sequences:\n",
    "            one_hot_seq = one_hot_encoding(seq,letter_to_int)\n",
    "            one_hot_seqs.append(one_hot_seq.flatten().numpy())\n",
    "\n",
    "        #print(\"one_hot_seqs size\",one_hot_seqs.shape)\n",
    "        # Create a dataframe with separate columns for each amino acid position\n",
    "        max_length = max(len(seq) for seq in sequences)\n",
    "        column_name = [f'aa{i}_{aa}' for aa in letter_to_int for i in range(1, max_length+1) ]\n",
    "\n",
    "\n",
    "        #print(\"one_hot_seqs shape\",one_hot_seqs.shape)\n",
    "        df = pd.DataFrame(one_hot_seqs,columns = column_name)\n",
    "        df  = df.fillna(value=0).astype(\"bool\")\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_infer_features_in_args() -> dict:\n",
    "        default_infer_features = dict(valid_raw_types=[R_OBJECT]) \n",
    "        print(default_infer_features)\n",
    "        return default_infer_features  # This limits input features to only integers. We can assume that the input to _fit_transform and _transform only contain the data post-applying this filter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f74d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_train = one_hot_Generator(verbosity=3,features_in=['seq'])\n",
    "# one_hot_train_data = one_hot_train.fit_transform(X=train_data)\n",
    "# one_hot_train_data  = one_hot_train_data.fillna(value=0, downcast='infer')\n",
    "# print(one_hot_train_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4770865f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valid_raw_types': ['object']}\n",
      "{'valid_raw_types': ['object']}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'R_INT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m\n\u001b[1;32m      4\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/user/mahaohui/autoML/autogluon_examples\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfeature_generator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m count_charge_Generator, net_charge_Generator, one_hot_Generator\n\u001b[1;32m      7\u001b[0m train_feature_generator \u001b[38;5;241m=\u001b[39m PipelineFeatureGenerator(\n\u001b[1;32m      8\u001b[0m     generators\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# Stage 1: Convert feature types to be the same as during fit. Does not need to be specified.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# Stage 2: Fill NaN values of data. Does not need to be specified.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         [  \u001b[38;5;66;03m# Stage 3: Add 5 to all int features and convert all object features to category features. Concatenate the outputs of each.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m             count_charge_Generator(),\n\u001b[1;32m     13\u001b[0m             net_charge_Generator(),\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;66;03m# one_hot_Generator(verbosity=3,features_in=['seq']),\u001b[39;00m\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;66;03m#OneHotEncoderFeatureGenerator(),\u001b[39;00m\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;66;03m#CategoryFeatureGenerator(),\u001b[39;00m\n\u001b[1;32m     17\u001b[0m             IdentityFeatureGenerator(infer_features_in_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m---> 18\u001b[0m                 valid_raw_types\u001b[38;5;241m=\u001b[39m[\u001b[43mR_INT\u001b[49m, R_FLOAT])),\n\u001b[1;32m     19\u001b[0m         ],\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;66;03m# Stage 4: Drop any features which are always the same value (useless). Does not need to be specified.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m      ],\n\u001b[1;32m     22\u001b[0m     verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m one_hot_all_data \u001b[38;5;241m=\u001b[39m train_feature_generator\u001b[38;5;241m.\u001b[39mfit_transform(X\u001b[38;5;241m=\u001b[39mconcatenated_df)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(one_hot_all_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'R_INT' is not defined"
     ]
    }
   ],
   "source": [
    "from autogluon.features.generators import CategoryFeatureGenerator, AsTypeFeatureGenerator, BulkFeatureGenerator, DropUniqueFeatureGenerator, FillNaFeatureGenerator, PipelineFeatureGenerator, OneHotEncoderFeatureGenerator,IdentityFeatureGenerator\n",
    "import copy\n",
    "\n",
    "\n",
    "train_feature_generator = PipelineFeatureGenerator(\n",
    "    generators=[\n",
    "        # Stage 1: Convert feature types to be the same as during fit. Does not need to be specified.\n",
    "        # Stage 2: Fill NaN values of data. Does not need to be specified.\n",
    "        [  # Stage 3: Add 5 to all int features and convert all object features to category features. Concatenate the outputs of each.\n",
    "            # count_charge_Generator(),\n",
    "            # net_charge_Generator(),\n",
    "            one_hot_Generator(verbosity=3,features_in=['seq']),\n",
    "            #OneHotEncoderFeatureGenerator(),\n",
    "            #CategoryFeatureGenerator(),\n",
    "            IdentityFeatureGenerator(infer_features_in_args=dict(\n",
    "                valid_raw_types=[R_INT, R_FLOAT])),\n",
    "        ],\n",
    "        # Stage 4: Drop any features which are always the same value (useless). Does not need to be specified.\n",
    "     ],\n",
    "    verbosity=3\n",
    ")\n",
    "\n",
    "one_hot_all_data = train_feature_generator.fit_transform(X=concatenated_df)\n",
    "print(one_hot_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77039f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa2_C            bool\n",
      "aa3_C            bool\n",
      "aa4_C            bool\n",
      "aa5_C            bool\n",
      "aa6_C            bool\n",
      "               ...   \n",
      "aa749_W          bool\n",
      "aa776_W          bool\n",
      "aa810_W          bool\n",
      "solubility       int8\n",
      "fold          float64\n",
      "Length: 13758, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_all_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7309bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       aa2_C  aa3_C  aa4_C  aa5_C  aa6_C  aa7_C  aa8_C  aa9_C  aa10_C  aa11_C  \\\n",
      "0      False  False  False  False  False  False  False  False   False   False   \n",
      "1      False  False  False  False  False  False  False  False   False   False   \n",
      "2      False  False  False  False  False  False  False  False   False   False   \n",
      "3      False  False  False  False  False  False  False  False   False   False   \n",
      "4      False  False  False  False  False  False  False  False   False   False   \n",
      "...      ...    ...    ...    ...    ...    ...    ...    ...     ...     ...   \n",
      "11219  False  False  False  False  False  False  False  False   False   False   \n",
      "11220  False  False  False  False  False  False  False  False   False   False   \n",
      "11221  False  False  False  False  False  False  False  False   False   False   \n",
      "11222  False  False  False  False  False  False  False  False   False   False   \n",
      "11223  False  False  False  False  False  False  False  False   False   False   \n",
      "\n",
      "       ...  aa576_W  aa627_W  aa635_W  aa739_W  aa745_W  aa749_W  aa776_W  \\\n",
      "0      ...    False    False    False    False    False    False    False   \n",
      "1      ...    False    False    False    False    False    False    False   \n",
      "2      ...    False    False    False    False    False    False    False   \n",
      "3      ...    False    False    False    False    False    False    False   \n",
      "4      ...    False    False    False    False    False    False    False   \n",
      "...    ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "11219  ...    False    False    False    False    False    False    False   \n",
      "11220  ...    False    False    False    False    False    False    False   \n",
      "11221  ...    False    False    False    False    False    False    False   \n",
      "11222  ...    False    False    False    False    False    False    False   \n",
      "11223  ...    False    False    False    False    False    False    False   \n",
      "\n",
      "       aa810_W  solubility  fold  \n",
      "0        False           1   0.0  \n",
      "1        False           1   1.0  \n",
      "2        False           1   1.0  \n",
      "3        False           1   1.0  \n",
      "4        False           1   2.0  \n",
      "...        ...         ...   ...  \n",
      "11219    False           1   0.0  \n",
      "11220    False           1   0.0  \n",
      "11221    False           1   0.0  \n",
      "11222    False           1   0.0  \n",
      "11223    False           0   3.0  \n",
      "\n",
      "[11224 rows x 13758 columns]\n",
      "      aa2_C  aa3_C  aa4_C  aa5_C  aa6_C  aa7_C  aa8_C  aa9_C  aa10_C  aa11_C  \\\n",
      "0     False  False  False  False  False  False  False  False   False   False   \n",
      "1     False  False  False  False  False  False  False  False   False   False   \n",
      "2     False  False  False  False  False  False  False  False   False   False   \n",
      "3     False  False  False  False  False  False  False  False   False   False   \n",
      "4     False  False  False  False  False  False  False  False   False   False   \n",
      "...     ...    ...    ...    ...    ...    ...    ...    ...     ...     ...   \n",
      "1318  False  False  False  False  False  False  False  False   False   False   \n",
      "1319  False  False  False  False  False  False  False  False   False   False   \n",
      "1320  False  False  False  False  False  False  False  False   False   False   \n",
      "1321  False  False  False  False  False  False  False  False   False   False   \n",
      "1322  False  False  False  False  False  False  False  False   False   False   \n",
      "\n",
      "      ...  aa576_W  aa627_W  aa635_W  aa739_W  aa745_W  aa749_W  aa776_W  \\\n",
      "0     ...    False    False    False    False    False    False    False   \n",
      "1     ...    False    False    False    False    False    False    False   \n",
      "2     ...    False    False    False    False    False    False    False   \n",
      "3     ...    False    False    False    False    False    False    False   \n",
      "4     ...    False    False    False    False    False    False    False   \n",
      "...   ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "1318  ...    False    False    False    False    False    False    False   \n",
      "1319  ...    False    False    False    False    False    False    False   \n",
      "1320  ...    False    False    False    False    False    False    False   \n",
      "1321  ...    False    False    False    False    False    False    False   \n",
      "1322  ...    False    False    False    False    False    False    False   \n",
      "\n",
      "      aa810_W  solubility  fold  \n",
      "0       False           0   NaN  \n",
      "1       False           0   NaN  \n",
      "2       False           0   NaN  \n",
      "3       False           1   NaN  \n",
      "4       False           1   NaN  \n",
      "...       ...         ...   ...  \n",
      "1318    False           1   NaN  \n",
      "1319    False           1   NaN  \n",
      "1320    False           1   NaN  \n",
      "1321    False           1   NaN  \n",
      "1322    False           1   NaN  \n",
      "\n",
      "[1323 rows x 13758 columns]\n"
     ]
    }
   ],
   "source": [
    "one_hot_train_data = one_hot_all_data[:len(train_data)]\n",
    "one_hot_test_data = one_hot_all_data[len(train_data):]\n",
    "print(one_hot_train_data)\n",
    "print(one_hot_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "827e6dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_valid_data1 = one_hot_train_data[one_hot_train_data[\"fold\"] ==0.0]\n",
    "one_hot_train_data1 = one_hot_train_data[one_hot_train_data[\"fold\"] !=0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcc578e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_train_data1 = one_hot_train_data1.drop([\"fold\"],axis=1)\n",
    "# one_hot_train_data1.astype(bool)                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e0147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ee07255",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_valid_data1 = one_hot_valid_data1.drop([\"fold\"],axis=1)\n",
    "# one_hot_valid_data1.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1611718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa2_C         bool\n",
      "aa3_C         bool\n",
      "aa4_C         bool\n",
      "aa5_C         bool\n",
      "aa6_C         bool\n",
      "              ... \n",
      "aa745_W       bool\n",
      "aa749_W       bool\n",
      "aa776_W       bool\n",
      "aa810_W       bool\n",
      "solubility    int8\n",
      "Length: 13757, dtype: object\n",
      "aa2_C         bool\n",
      "aa3_C         bool\n",
      "aa4_C         bool\n",
      "aa5_C         bool\n",
      "aa6_C         bool\n",
      "              ... \n",
      "aa745_W       bool\n",
      "aa749_W       bool\n",
      "aa776_W       bool\n",
      "aa810_W       bool\n",
      "solubility    int8\n",
      "Length: 13757, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_train_data1.dtypes)\n",
    "print(one_hot_valid_data1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42ca1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO THE ENCODING BEFOTRE SPLITING TRAIN VALID!!!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28c8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b14ee16a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230620_022326/\"\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (8281 samples, 113.99 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230620_022326/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.0\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 10 13:32:12 UTC 2021\n",
      "Train Data Rows:    8281\n",
      "Train Data Columns: 13756\n",
      "Tuning Data Rows:    2943\n",
      "Tuning Data Columns: 13756\n",
      "Label Column: solubility\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting IdentityFeatureGenerator...\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "Data preprocessing and feature engineering runtime = 7.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'precision'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
      "Fitting model: KNeighborsDist ...\n",
      "\tNo valid features to train KNeighborsDist... Skipping this model.\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.7131\t = Validation score   (precision)\n",
      "\t24.85s\t = Training   runtime\n",
      "\t0.53s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.7131\t = Validation score   (precision)\n",
      "\t21.64s\t = Training   runtime\n",
      "\t0.54s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7053\t = Validation score   (precision)\n",
      "\t17.95s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.7022\t = Validation score   (precision)\n",
      "\t17.59s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.716\t = Validation score   (precision)\n",
      "\t280.47s\t = Training   runtime\n",
      "\t12.62s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.706\t = Validation score   (precision)\n",
      "\t17.43s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.701\t = Validation score   (precision)\n",
      "\t17.85s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\t0.7451\t = Validation score   (precision)\n",
      "\t3069.43s\t = Training   runtime\n",
      "\t71.98s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.7236\t = Validation score   (precision)\n",
      "\t134.08s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\tlist index out of range\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1502, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1447, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 703, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    self._train_net(train_dataset=train_dataset,\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 298, in _train_net\n",
      "    loss = self.model.compute_loss(data_batch, **loss_kwargs)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/torch_network_modules.py\", line 231, in compute_loss\n",
      "    predict_data = self(data_batch)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/user/mahaohui/miniconda3/envs/autoML/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/torch_network_modules.py\", line 169, in forward\n",
      "    input_data = input_data[0]\n",
      "IndexError: list index out of range\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.7114\t = Validation score   (precision)\n",
      "\t29.18s\t = Training   runtime\n",
      "\t0.53s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.7452\t = Validation score   (precision)\n",
      "\t1.95s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3766.33s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230620_022326/\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label='solubility',eval_metric=\"precision\").fit(train_data=one_hot_train_data1, tuning_data=one_hot_valid_data1, feature_generator=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a9219f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6659663865546218,\n",
       " 'accuracy': 0.6024187452758881,\n",
       " 'balanced_accuracy': 0.5459232300087407,\n",
       " 'mcc': 0.09835253929687982,\n",
       " 'roc_auc': 0.5585503281465277,\n",
       " 'f1': 0.7068004459308808,\n",
       " 'recall': 0.7529691211401425}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(one_hot_test_data, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ef4fcb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7452443118239462,\n",
       " 'accuracy': 0.745837580699966,\n",
       " 'balanced_accuracy': 0.5961780615167673,\n",
       " 'mcc': 0.3092416007181366,\n",
       " 'roc_auc': 0.6757819041113999,\n",
       " 'f1': 0.8423271500843171,\n",
       " 'recall': 0.9684924866698982}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(one_hot_valid_data1, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c1b5e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
